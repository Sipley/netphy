
R version 3.2.3 (2015-12-10) -- "Wooden Christmas-Tree"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

This is vegan 2.3-3

 # maps v3.1: updated 'world': all lakes moved to separate new #
 # 'lakes' database. Type '?world' or 'news(package="maps")'.  #


foreach: simple, scalable parallel programming from Revolution Analytics
Use Revolution R for scalability, fault tolerance and more.
http://www.revolutionanalytics.com
During startup - Warning message:
package 'ggplot2' was built under R version 3.2.4
> > options(STERM='iESS', str.dendrogram.last="'", editor='emacsclient', show.error.locations=TRUE)
> library(geiger)
> library(devtools)

Attaching package: 'devtools'

The following object is masked from 'package:permute':

    check

Warning message:
package 'devtools' was built under R version 3.2.5
> install_github("lukejharmon/netphy")
Skipping install for github remote, the SHA1 (c3f3910c) has not changed since last install.
  Use `force = TRUE` to force installation
> library(netphy)
Error in library(netphy) : there is no package called 'netphy'
> install_github("lukejharmon/netphy", force = TRUE)
Downloading GitHub repo lukejharmon/netphy@master
from URL https://api.github.com/repos/lukejharmon/netphy/zipball/master
Installing np
'/Library/Frameworks/R.framework/Resources/bin/R' --no-site-file --no-environ  \
  --no-save --no-restore --quiet CMD INSTALL  \
  '/private/var/folders/gd/h3kd1r9x6zx4qvz1bcf3st_c0000gn/T/RtmpalTQuE/devtools261368400cf8/lukejharmon-netphy-c3f3910'  \
  --library='/Library/Frameworks/R.framework/Versions/3.2/Resources/library'  \
  --install-tests

* installing *source* package 'np' ...
** R
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (np)
> library(netphy)
Error in library(netphy) : there is no package called 'netphy'
> ls()
character(0)
> search()
 [1] ".GlobalEnv"          "package:devtools"    "ESSR"
 [4] "package:diversitree" "package:gridExtra"   "package:ggplot2"
 [7] "package:doMC"        "package:parallel"    "package:iterators"
[10] "package:foreach"     "package:plyr"        "package:phytools"
[13] "package:maps"        "package:picante"     "package:nlme"
[16] "package:vegan"       "package:lattice"     "package:permute"
[19] "package:TreeSim"     "package:laser"       "package:geiger"
[22] "package:ape"         "package:stats"       "package:graphics"
[25] "package:grDevices"   "package:utils"       "package:datasets"
[28] "package:methods"     "Autoloads"           "package:base"
> list.files("./R/")
[1] "MatrixExp.eig.R"   "branchLike.R"      "fitPhyloNetwork.R"
[4] "internals.R"       "logspace_add.R"    "logspace_sum.R"
[7] "simPhyloNetwork.R"
> lapply(as.list(list.files("./R/")), source)
Error in file(filename, "r", encoding = encoding) :
  cannot open the connection
In addition: Warning message:
In file(filename, "r", encoding = encoding) :
  cannot open file 'MatrixExp.eig.R': No such file or directory
> lapply(paste0("./R/",as.list(list.files("./R/"))), source)
[[1]]
[[1]]$value
function (Q)
{
    tmp <- eigen(Q, symmetric = FALSE)
    P1 <- tmp$vectors %*% diag(exp(tmp$values)) %*% solve(tmp$vectors)
    return(P1)
}

[[1]]$visible
[1] FALSE


[[2]]
[[2]]$value
function (tip.like, bl, q)
{
    nb.states <- length(tip.like)
    r <- rep(0, nb.states)
    p <- MatrixExp.eig(q * bl)
    for (i in 1:nb.states) r[i] <- logspace_sum(log(p[i, ]) +
        tip.like)
    return(r)
}

[[2]]$visible
[1] FALSE


[[3]]
[[3]]$value
function (phy, network, pars)
{
    if (sum(phy$edge.length == 0) > 0)
        cat("Function probably will not work when tree has zero-length branches\n\n")
    mm <- match(phy$tip.label, rownames(network))
    network <- network[mm, mm]
    nNodes <- max(phy$edge)
    nTips <- length(phy$tip.label)
    lik <- list()
    q01 <- pars[1]
    q10 <- pars[2]
    pSpec <- pars[3]
    if (pSpec > 1)
        pSpec <- 1
    qMatrix <- rbind(c(-q01, q01), c(q10, -q10))
    for (i in 1:nTips) {
        theTip <- phy$tip.label[i]
        theRow <- which(rownames(network) == theTip)
        interact <- network[theRow, ]
        rr <- matrix(nrow = nTips, ncol = 2)
        rownames(rr) <- phy$tip.label
        colnames(rr) <- c(0, 1)
        for (j in 1:nTips) if (interact[j] == 0)
            rr[j, ] <- c(0, -Inf)
        else rr[j, ] <- c(-Inf, 0)
        lik[[i]] <- rr
        names(lik)[i] <- theTip
    }
    currTree <- phy
    cumlnL <- 0
    while (1) {
        bt <- branching.times(currTree)
        recentNode <- names(bt)[which(bt == min(bt))][1]
        daughter <- tips(currTree, as.numeric(recentNode))
        timeInterval <- bt[recentNode]
        d1 <- which(names(lik) == daughter[1])
        d1L <- lik[[d1]]
        d2 <- which(names(lik) == daughter[2])
        d2L <- lik[[d2]]
        dCon <- d1L[daughter[2], ]
        dNode <- branchLike(dCon, timeInterval, qMatrix)
        dd <- dNode + log(c(1 - pSpec, pSpec))
        cumlnL <- cumlnL + -logspace_sum(dd)
        if (length(currTree$tip.label) == 2)
            break
        currTree <- drop.tip(currTree, daughter[1])
        oldLik <- lik
        lik <- list()
        nn <- length(currTree$tip.label)
        for (i in 1:nn) {
            theTip <- currTree$tip.label[i]
            rr <- matrix(nrow = nn, ncol = 2)
            rownames(rr) <- currTree$tip.label
            colnames(rr) <- c(0, 1)
            if (theTip %in% daughter) {
                xx <- which(names(oldLik) == daughter[1])
                ol1 <- oldLik[[xx]]
                xx <- which(names(oldLik) == daughter[2])
                ol2 <- oldLik[[xx]]
                for (j in 1:nrow(ol1)) {
                  if (rownames(ol1)[j] != daughter[1]) {
                    l1 <- branchLike(ol1[j, ], timeInterval,
                      qMatrix)
                    l2 <- branchLike(ol2[j, ], timeInterval,
                      qMatrix)
                    rr[rownames(ol1)[j], ] <- l1 + l2
                  }
                }
            }
            else {
                xx <- which(names(oldLik) == theTip)
                theOldLik <- oldLik[[xx]]
                lMerg <- theOldLik[daughter, ]
                l1 <- branchLike(lMerg[1, ], timeInterval, qMatrix)
                l2 <- branchLike(lMerg[2, ], timeInterval, qMatrix)
                rr[daughter[2], ] <- l1 + l2
                theRest <- rownames(rr)[which(!(rownames(rr) %in%
                  daughter))]
                for (tt in theRest) {
                  lold <- theOldLik[tt, ]
                  lnew <- branchLike(lold, timeInterval, qMatrix)
                  rr[tt, ] <- lnew
                }
            }
            rr[theTip, ] <- c(0, -Inf)
            lik[[i]] <- rr
            names(lik)[i] <- theTip
            nEdge <- which(currTree$edge[, 2] == i)
            currTree$edge.length[nEdge] <- currTree$edge.length[nEdge] -
                timeInterval
        }
        if (length(lik) == 80)
            break
    }
    return(cumlnL)
}

[[3]]$visible
[1] FALSE


[[4]]
[[4]]$value
function (tip.like, bl, q)
{
    nb.states <- length(tip.like)
    r <- rep(0, nb.states)
    p <- MatrixExp.eig(q * bl)
    for (i in 1:nb.states) r[i] <- logspace_sum(log(p[i, ]) +
        tip.like)
    return(r)
}

[[4]]$visible
[1] FALSE


[[5]]
[[5]]$value
function (logx, logy)
{
    if (logx == -Inf)
        return(logy)
    else max(logx, logy) + log1p(exp(-abs(logx - logy)))
}

[[5]]$visible
[1] FALSE


[[6]]
[[6]]$value
function (logx)
{
    r <- logx[1]
    if (length(logx) > 1)
        for (i in 2:length(logx)) r <- logspace_add(r, logx[i])
    r
}

[[6]]$visible
[1] FALSE


[[7]]
[[7]]$value
function (phy, qRate, sProb)
{
    bt <- sort(branching.times(phy), decr = T)
    nb <- dim(phy$edge)[1]
    interactionMatrix <- matrix(nrow = nb, ncol = nb)
    interactionMatrix[] <- 0
    currentEdges <- numeric()
    rootNumber <- as.numeric(names(bt)[1])
    qMatrix <- rbind(c(-1, 1), c(1, -1)) * qRate
    for (i in 1:length(bt)) {
        thisEdge <- as.numeric(names(bt)[i])
        ancestorRow <- which(phy$edge[, 2] == thisEdge)
        descendantRow <- which(phy$edge[, 1] == thisEdge)
        if (length(ancestorRow) == 0) {
            r <- runif(1)
            if (r < sProb) {
                interactionMatrix[descendantRow, descendantRow] <- 1
                diag(interactionMatrix) <- 0
            }
            currentEdges <- c(currentEdges, descendantRow)
        }
        else {
            timeSpan <- bt[i - 1] - bt[i]
            tProb <- MatrixExp.eig(qMatrix * timeSpan)
            cn <- length(currentEdges)
            for (j in 1:(cn - 1)) for (k in (j + 1):cn) {
                e1 <- currentEdges[j]
                e2 <- currentEdges[k]
                currState <- interactionMatrix[e1, e2]
                p0 <- tProb[currState + 1, 1]
                r <- runif(1)
                if (r < p0) {
                  interactionMatrix[e1, e2] <- 0
                  interactionMatrix[e2, e1] <- 0
                }
                else {
                  interactionMatrix[e1, e2] <- 1
                  interactionMatrix[e2, e1] <- 1
                }
            }
            toCut <- which(currentEdges == ancestorRow)
            currentEdges <- currentEdges[-toCut]
            currentEdges <- c(currentEdges, descendantRow)
            for (j in 1:length(descendantRow)) {
                interactionMatrix[, descendantRow[j]] <- interactionMatrix[,
                  ancestorRow]
                interactionMatrix[descendantRow[j], ] <- interactionMatrix[ancestorRow,
                  ]
                diag(interactionMatrix) <- 0
            }
            r <- runif(1)
            if (r < sProb) {
                interactionMatrix[descendantRow, descendantRow] <- 1
            }
        }
    }
    nTaxa <- length(phy$tip.label)
    tips <- which(phy$edge[, 2] <= nTaxa)
    oo <- phy$edge[tips, 2]
    res <- interactionMatrix[tips, tips]
    rownames(res) <- phy$tip.label[oo]
    colnames(res) <- phy$tip.label[oo]
    return(res)
}

[[7]]$visible
[1] FALSE


> ls()
[1] "MatrixExp.eig"   "branchLike"      "fitPhyloNetwork" "logspace_add"
[5] "logspace_sum"    "simPhyloNetwork"
> tt<-sim.bdtree(b=1, d=0, stop="taxa", n = 60)
> tt <- sim.bdtree(b = runif(1, spProb[1], spProb[2]), d = 0, stop = "taxa", n = round(runif(1, tProb[1], tProb[2])))
Error in runif(1, tProb[1], tProb[2]) : object 'tProb' not found
> sProb <- c(0,2)  # pSpec = probability of speciation within the network
> qProb <- c(0,1)  # transition probability between interacting and not-interacting
> tProb <- c(10,100)  # total number of taxa in the tree
> spProb <- c(0,2)  # lambda for tree simulation
> tt <- sim.bdtree(b = runif(1, spProb[1], spProb[2]), d = 0, stop = "taxa", n = round(runif(1, tProb[1], tProb[2])))
> tt

Phylogenetic tree with 36 tips and 35 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> pSpec <- runif(1, sProb[1], sProb[2])
> q01 <- runif(1, qProb[1], qProb[2])
> q10 <- runif(1, qProb[1], qProb[2])
> lambda <- runif(1, spProb[1], spProb[2])
> ntaxa <- round(runif(1, tProb[1], tProb[2]))
> tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
> tt

Phylogenetic tree with 71 tips and 70 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
> net
    s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 s19 s20 s21
s1   0  0  0  0  0  0  0  0  0   1   1   0   0   0   0   1   1   1   0   0   0
s2   0  0  1  1  0  1  1  1  1   1   1   1   1   1   1   1   1   1   1   1   1
s3   0  1  1  1  1  1  1  1  1   1   1   1   1   0   0   0   1   1   1   1   1
s4   0  1  1  1  1  1  1  1  1   1   1   1   1   0   0   0   1   1   1   1   1
s5   0  0  1  1  0  1  1  1  1   1   1   1   1   1   1   1   1   1   1   1   1
s6   0  1  1  1  1  0  0  0  0   0   0   0   0   0   0   0   0   0   1   1   1
s7   0  1  1  1  1  0  0  0  0   0   0   0   0   0   0   0   0   0   1   1   1
s8   0  1  1  1  1  0  0  0  0   0   0   0   0   0   0   0   1   1   1   1   1
s9   0  1  1  1  1  0  0  0  0   0   0   1   1   0   0   0   0   0   0   0   0
s10  1  1  1  1  1  0  0  0  0   0   0   0   0   1   1   1   1   1   1   0   1
s11  1  1  1  1  1  0  0  0  0   0   0   0   0   1   1   1   1   0   1   1   1
s12  0  1  1  1  1  0  0  0  1   0   0   0   0   1   1   1   1   1   1   1   1
s13  0  1  1  1  1  0  0  0  1   0   0   0   0   1   1   1   1   1   1   1   1
s14  0  1  0  0  1  0  0  0  0   1   1   1   1   0   0   1   1   1   1   1   1
s15  0  1  0  0  1  0  0  0  0   1   1   1   1   0   0   1   1   1   1   1   1
s16  1  1  0  0  1  0  0  0  0   1   1   1   1   1   1   0   1   1   1   1   1
s17  1  1  1  1  1  0  0  1  0   1   1   1   1   1   1   1   0   1   1   1   1
s18  1  1  1  1  1  0  0  1  0   1   0   1   1   1   1   1   1   0   1   1   1
s19  0  1  1  1  1  1  1  1  0   1   1   1   1   1   1   1   1   1   0   1   1
s20  0  1  1  1  1  1  1  1  0   0   1   1   1   1   1   1   1   1   1   0   1
s21  0  1  1  1  1  1  1  1  0   1   1   1   1   1   1   1   1   1   1   1   0
s22  0  0  1  1  1  0  0  1  0   1   1   1   1   1   1   1   1   1   1   0   0
s23  0  0  1  1  1  0  0  1  1   1   1   1   1   1   0   1   1   1   1   0   0
s24  0  1  1  1  1  0  0  1  1   1   0   1   1   1   1   1   1   1   1   1   1
s25  0  1  1  1  1  0  0  1  1   1   1   1   1   1   1   1   1   1   1   1   1
s26  0  1  1  1  1  0  0  1  1   1   0   1   1   1   1   1   1   1   1   0   0
s27  0  1  1  1  1  0  0  1  1   1   1   1   1   1   1   1   1   1   0   1   1
s28  0  1  1  1  1  0  0  1  1   1   1   0   0   1   1   1   1   1   1   1   1
s29  0  0  0  0  1  0  0  1  0   0   0   0   0   1   1   1   1   1   1   1   1
s30  0  0  0  0  1  0  0  1  0   1   1   1   1   1   1   1   1   0   1   1   1
s31  0  1  1  1  0  1  1  1  1   1   1   1   1   0   1   0   1   0   1   1   1
s32  1  1  1  1  0  1  1  1  1   1   0   1   1   1   1   0   1   0   1   1   1
s33  0  0  1  1  0  1  1  0  0   1   1   1   1   1   1   1   0   0   0   1   1
s34  0  0  1  1  0  1  1  0  0   1   1   1   1   1   1   1   0   0   0   0   1
s35  0  0  0  0  1  1  1  0  0   1   1   1   1   0   0   1   1   1   1   1   1
s36  0  0  0  0  1  1  1  0  0   1   1   1   1   0   0   1   1   1   1   1   1
s37  0  0  0  0  1  1  1  0  0   1   1   1   1   0   0   1   1   1   0   1   1
s38  0  0  0  0  1  1  1  0  0   1   1   1   1   1   1   1   1   1   1   1   1
s39  0  0  0  0  1  1  1  0  0   1   1   1   1   1   1   1   1   1   1   1   1
s40  1  0  0  0  0  0  0  0  0   1   1   1   1   1   1   1   0   1   1   1   1
s41  1  0  0  0  0  0  0  0  0   1   1   1   1   1   1   1   1   1   1   1   1
s42  1  0  0  0  0  0  0  0  0   1   1   1   1   1   1   1   1   1   1   0   1
s43  0  0  0  0  0  0  0  0  1   1   1   1   1   1   1   1   1   1   1   1   1
s44  0  0  0  0  0  0  0  0  1   1   1   1   1   1   1   1   1   1   1   1   1
s45  0  0  0  0  0  1  1  1  0   0   0   0   0   1   1   1   1   1   1   1   1
s46  0  0  0  0  1  1  1  1  1   0   0   0   0   1   1   1   1   0   1   1   1
s47  0  0  0  0  1  1  1  1  1   0   0   0   0   1   1   1   1   0   1   1   1
s48  0  0  0  0  1  1  1  1  1   1   1   0   0   1   1   1   1   1   0   1   1
s49  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   0   0
s50  0  0  0  0  1  1  1  0  1   0   0   0   0   0   0   0   0   0   0   0   0
s51  0  0  0  0  1  1  1  0  1   0   0   0   0   0   0   0   0   0   0   0   0
s52  0  0  0  0  1  0  0  1  0   0   0   0   0   0   0   1   1   0   1   1   1
s53  0  0  0  0  0  0  0  0  0   0   1   1   1   1   1   1   0   0   0   0   0
s54  0  0  0  0  0  0  0  0  0   0   1   1   1   1   1   1   0   0   1   0   0
s55  0  0  0  0  0  0  0  0  0   0   1   1   0   1   1   1   0   0   0   0   0
s56  0  0  0  0  0  0  0  0  0   0   1   1   1   1   1   1   0   0   0   0   0
s57  1  0  0  0  0  0  0  0  0   0   0   0   0   0   0   1   1   0   0   0   0
s58  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   1   0   0   0   0   0
s59  0  0  0  0  1  0  0  0  0   0   0   0   0   0   0   1   0   0   0   0   0
s60  1  1  1  1  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   0   0
s61  0  0  1  1  0  1  1  0  0   1   1   1   1   0   0   1   0   0   0   0   0
s62  0  1  1  1  0  0  0  0  0   0   0   0   0   0   0   0   0   0   1   0   0
s63  1  1  1  1  0  0  0  1  1   1   0   1   1   0   0   1   1   1   0   1   1
s64  0  1  1  1  1  0  1  1  1   1   1   1   1   1   1   1   0   1   0   0   0
s65  1  1  1  1  1  0  0  1  1   0   0   0   0   1   1   0   1   0   1   1   1
s66  1  1  0  0  1  1  1  1  1   0   1   1   1   1   1   0   1   1   1   1   1
s67  1  0  1  1  1  1  1  1  0   1   1   1   1   1   1   1   1   0   0   1   1
s68  0  0  1  1  1  1  1  0  1   1   1   1   1   1   1   1   0   0   1   1   1
s69  0  1  1  1  0  0  0  1  1   1   1   1   1   1   1   1   0   1   1   1   1
s70  1  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   0   0
s71  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   1   1   0   0   0   0
    s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 s36 s37 s38 s39 s40
s1    0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   1
s2    0   0   1   1   1   1   1   0   0   1   1   0   0   0   0   0   0   0   0
s3    1   1   1   1   1   1   1   0   0   1   1   1   1   0   0   0   0   0   0
s4    1   1   1   1   1   1   1   0   0   1   1   1   1   0   0   0   0   0   0
s5    1   1   1   1   1   1   1   1   1   0   0   0   0   1   1   1   1   1   0
s6    0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   1   0
s7    0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   1   0
s8    1   1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0
s9    0   1   1   1   1   1   1   0   0   1   1   0   0   0   0   0   0   0   0
s10   1   1   1   1   1   1   1   0   1   1   1   1   1   1   1   1   1   1   1
s11   1   1   0   1   0   1   1   0   1   1   0   1   1   1   1   1   1   1   1
s12   1   1   1   1   1   1   0   0   1   1   1   1   1   1   1   1   1   1   1
s13   1   1   1   1   1   1   0   0   1   1   1   1   1   1   1   1   1   1   1
s14   1   1   1   1   1   1   1   1   1   0   1   1   1   0   0   0   1   1   1
s15   1   0   1   1   1   1   1   1   1   1   1   1   1   0   0   0   1   1   1
s16   1   1   1   1   1   1   1   1   1   0   0   1   1   1   1   1   1   1   1
s17   1   1   1   1   1   1   1   1   1   1   1   0   0   1   1   1   1   1   0
s18   1   1   1   1   1   1   1   1   0   0   0   0   0   1   1   1   1   1   1
s19   1   1   1   1   1   0   1   1   1   1   1   0   0   1   1   0   1   1   1
s20   0   0   1   1   0   1   1   1   1   1   1   1   0   1   1   1   1   1   1
s21   0   0   1   1   0   1   1   1   1   1   1   1   1   1   1   1   1   1   1
s22   0   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0   1   1
s23   1   0   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
s24   1   1   0   0   0   1   1   0   0   1   1   1   1   1   1   1   1   1   0
s25   1   1   0   0   1   1   1   0   0   1   1   0   0   1   1   1   0   0   1
s26   1   1   0   1   0   1   1   0   0   1   1   1   1   1   1   1   1   1   0
s27   1   1   1   1   1   0   1   1   0   1   0   1   1   1   1   1   1   1   1
s28   1   1   1   1   1   1   0   0   0   1   1   1   1   1   1   1   1   1   1
s29   1   1   0   0   0   1   0   0   1   1   1   1   1   1   1   1   1   1   1
s30   1   1   0   0   0   0   0   1   0   1   1   1   1   1   1   1   1   1   1
s31   1   1   1   1   1   1   1   1   1   0   1   1   1   1   1   1   1   1   0
s32   1   1   1   1   1   0   1   1   1   1   0   1   1   1   1   1   0   0   0
s33   1   1   1   0   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0
s34   1   1   1   0   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0
s35   1   1   1   1   1   1   1   1   1   1   1   0   0   0   1   1   0   0   0
s36   1   1   1   1   1   1   1   1   1   1   1   0   0   1   0   1   0   1   0
s37   1   1   1   1   1   1   1   1   1   1   1   0   0   1   1   0   1   1   0
s38   0   1   1   0   1   1   1   1   1   1   0   0   0   0   0   1   0   0   0
s39   1   1   1   0   1   1   1   1   1   1   0   0   0   0   1   1   0   0   0
s40   1   1   0   1   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0
s41   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   0   0
s42   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   0   1
s43   0   0   0   0   0   1   1   1   1   1   1   1   1   1   0   0   0   0   0
s44   0   0   0   0   0   1   1   1   1   1   1   1   1   1   0   0   0   0   0
s45   1   1   1   0   1   1   1   0   0   1   1   1   1   0   0   0   0   0   0
s46   1   1   1   1   0   1   1   0   0   1   1   1   1   0   1   0   0   0   0
s47   1   1   1   1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0
s48   0   0   1   1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0
s49   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0
s50   0   0   1   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   1
s51   0   0   1   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   1
s52   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   1
s53   0   0   0   0   0   0   0   1   1   0   0   0   0   1   0   0   0   0   0
s54   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0
s55   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   1   0   0
s56   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   1   0   0
s57   1   1   0   1   1   0   0   0   0   0   0   1   1   0   0   0   0   0   0
s58   1   1   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0
s59   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0
s60   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   1
s61   0   1   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   1
s62   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s63   0   0   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0   1
s64   0   0   1   1   1   1   1   1   0   1   1   1   1   1   1   1   0   0   1
s65   1   1   0   0   0   1   1   0   1   0   0   0   0   1   1   1   1   1   1
s66   1   1   1   1   1   1   1   1   1   0   0   1   1   1   1   1   1   1   1
s67   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   1   1   1
s68   1   0   1   1   1   0   1   1   1   1   1   1   1   0   0   0   0   0   1
s69   1   1   1   1   1   1   1   1   0   1   1   0   0   0   0   0   0   0   1
s70   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   0
s71   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   0   1   1   1
    s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 s54 s55 s56 s57 s58 s59
s1    1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0
s2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s5    0   0   0   0   0   1   1   1   0   1   1   1   0   0   0   0   0   0   1
s6    0   0   0   0   1   1   1   1   0   1   1   0   0   0   0   0   0   0   0
s7    0   0   0   0   1   1   1   1   0   1   1   0   0   0   0   0   0   0   0
s8    0   0   0   0   1   1   1   1   0   0   0   1   0   0   0   0   0   0   0
s9    0   0   1   1   0   1   1   1   0   1   1   0   0   0   0   0   0   0   0
s10   1   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0
s11   1   1   1   1   0   0   0   1   0   0   0   0   1   1   1   1   0   0   0
s12   1   1   1   1   0   0   0   0   0   0   0   0   1   1   1   1   0   0   0
s13   1   1   1   1   0   0   0   0   0   0   0   0   1   1   0   1   0   0   0
s14   1   1   1   1   1   1   1   1   0   0   0   0   1   1   1   1   0   0   0
s15   1   1   1   1   1   1   1   1   0   0   0   0   1   1   1   1   0   0   0
s16   1   1   1   1   1   1   1   1   0   0   0   1   1   1   1   1   1   1   1
s17   1   1   1   1   1   1   1   1   0   0   0   1   0   0   0   0   1   0   0
s18   1   1   1   1   1   0   0   1   0   0   0   0   0   0   0   0   0   0   0
s19   1   1   1   1   1   1   1   0   0   0   0   1   0   1   0   0   0   0   0
s20   1   0   1   1   1   1   1   1   0   0   0   1   0   0   0   0   0   0   0
s21   1   1   1   1   1   1   1   1   0   0   0   1   0   0   0   0   0   0   0
s22   1   1   0   0   1   1   1   0   0   0   0   1   0   0   0   0   1   1   0
s23   1   1   0   0   1   1   1   0   0   0   0   1   0   0   0   0   1   1   0
s24   1   1   0   0   1   1   1   1   0   1   1   1   0   0   0   0   0   1   0
s25   1   1   0   0   0   1   1   1   0   0   0   1   0   0   0   0   1   1   0
s26   1   1   0   0   1   0   1   1   0   0   0   1   0   0   0   0   1   1   0
s27   1   1   1   1   1   1   1   1   0   0   0   1   0   0   0   0   0   0   0
s28   1   1   1   1   1   1   1   1   0   1   1   1   0   0   0   0   0   0   0
s29   1   1   1   1   0   0   1   1   0   1   1   0   1   1   1   1   0   0   0
s30   1   1   1   1   0   0   1   1   0   0   0   0   1   1   1   1   0   1   0
s31   0   0   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   0   1
s32   0   0   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   0   0
s33   0   0   1   1   1   1   1   1   0   0   0   0   0   0   0   0   1   0   0
s34   0   0   1   1   1   1   1   1   0   0   0   0   0   0   0   0   1   0   0
s35   0   0   1   1   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0
s36   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
s37   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s38   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0
s39   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s40   0   1   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0
s41   0   1   0   0   0   0   0   0   1   1   1   1   0   0   0   0   0   0   0
s42   1   0   0   0   0   0   0   0   0   1   1   1   0   0   0   0   0   0   0
s43   0   0   0   1   0   0   0   0   0   0   0   1   0   0   0   0   1   1   0
s44   0   0   1   0   0   0   0   0   0   0   0   1   0   0   0   0   1   1   0
s45   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0
s46   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0
s47   0   0   0   0   1   1   0   1   0   0   0   0   0   0   0   0   0   0   0
s48   0   0   0   0   1   1   1   0   0   0   0   0   0   1   0   0   0   0   0
s49   1   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0   0   0
s50   1   1   0   0   0   0   0   0   0   0   1   0   1   1   1   1   1   1   1
s51   1   1   0   0   0   0   0   0   0   1   0   0   1   1   1   1   1   1   1
s52   1   1   1   1   0   0   0   0   1   0   0   0   1   1   1   1   1   0   1
s53   0   0   0   0   0   0   0   0   1   1   1   1   0   0   1   1   1   1   1
s54   0   0   0   0   0   0   0   1   0   1   1   1   0   0   1   1   1   1   1
s55   0   0   0   0   0   0   0   0   0   1   1   1   1   1   0   1   1   1   1
s56   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   0   1   1   1
s57   0   0   1   1   0   0   0   0   0   1   1   1   1   1   1   1   0   1   1
s58   0   0   1   1   0   0   0   0   0   1   1   0   1   1   1   1   1   0   0
s59   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   0   0
s60   0   1   0   0   0   0   0   0   0   0   0   1   1   1   1   1   0   0   0
s61   1   1   1   1   0   1   1   0   1   1   1   1   1   1   1   1   0   0   0
s62   0   0   0   0   1   1   1   1   0   0   0   0   0   1   1   1   1   1   1
s63   0   0   1   1   1   1   1   0   1   1   1   1   0   0   0   0   1   1   1
s64   1   1   0   0   1   1   1   1   1   1   1   0   1   1   1   1   1   1   1
s65   1   1   1   1   1   1   1   1   1   1   1   0   1   1   1   1   0   0   0
s66   1   1   0   0   1   1   0   1   1   1   1   1   1   1   1   1   0   0   0
s67   1   1   1   1   1   1   1   1   1   0   0   1   1   1   1   1   1   1   1
s68   1   0   1   1   1   1   1   1   1   1   1   1   1   1   1   0   1   1   1
s69   1   1   1   1   0   0   1   0   0   1   1   1   1   1   1   1   1   1   1
s70   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1   1   1
s71   1   1   1   1   0   0   0   0   1   1   1   1   1   1   1   1   1   1   1
    s60 s61 s62 s63 s64 s65 s66 s67 s68 s69 s70 s71
s1    1   0   0   1   0   1   1   1   0   0   1   0
s2    1   0   1   1   1   1   1   0   0   1   0   0
s3    1   1   1   1   1   1   0   1   1   1   0   0
s4    1   1   1   1   1   1   0   1   1   1   0   0
s5    0   0   0   0   1   1   1   1   1   0   0   0
s6    0   1   0   0   0   0   1   1   1   0   0   0
s7    0   1   0   0   1   0   1   1   1   0   0   0
s8    0   0   0   1   1   1   1   1   0   1   0   0
s9    0   0   0   1   1   1   1   0   1   1   0   0
s10   0   1   0   1   1   0   0   1   1   1   0   0
s11   0   1   0   0   1   0   1   1   1   1   0   0
s12   0   1   0   1   1   0   1   1   1   1   0   0
s13   0   1   0   1   1   0   1   1   1   1   0   0
s14   0   0   0   0   1   1   1   1   1   1   0   0
s15   0   0   0   0   1   1   1   1   1   1   0   0
s16   0   1   0   1   1   0   0   1   1   1   0   1
s17   0   0   0   1   0   1   1   1   0   0   0   1
s18   0   0   0   1   1   0   1   0   0   1   0   0
s19   0   0   1   0   0   1   1   0   1   1   0   0
s20   0   0   0   1   0   1   1   1   1   1   0   0
s21   0   0   0   1   0   1   1   1   1   1   0   0
s22   0   0   0   0   0   1   1   1   1   1   0   0
s23   0   1   0   0   0   1   1   1   0   1   0   0
s24   0   0   0   1   1   0   1   1   1   1   0   0
s25   0   1   0   1   1   0   1   1   1   1   0   0
s26   0   1   0   1   1   0   1   1   1   1   0   0
s27   1   0   0   1   1   1   1   1   0   1   0   0
s28   0   0   0   1   1   1   1   1   1   1   0   0
s29   0   0   0   1   1   0   1   1   1   1   0   0
s30   0   0   0   1   0   1   1   1   1   0   0   1
s31   0   0   0   1   1   0   0   0   1   1   0   1
s32   0   0   0   1   1   0   0   0   1   1   0   1
s33   0   0   0   1   1   0   1   0   1   0   1   1
s34   0   0   0   1   1   0   1   0   1   0   1   1
s35   0   0   0   1   1   1   1   0   0   0   1   1
s36   0   0   0   1   1   1   1   0   0   0   1   1
s37   0   0   0   1   1   1   1   0   0   0   1   0
s38   0   0   0   1   0   1   1   1   0   0   1   1
s39   0   0   0   0   0   1   1   1   0   0   1   1
s40   1   1   0   1   1   1   1   1   1   1   0   1
s41   0   1   0   0   1   1   1   1   1   1   0   1
s42   1   1   0   0   1   1   1   1   0   1   0   1
s43   0   1   0   1   0   1   0   1   1   1   0   1
s44   0   1   0   1   0   1   0   1   1   1   0   1
s45   0   0   1   1   1   1   1   1   1   0   0   0
s46   0   1   1   1   1   1   1   1   1   0   0   0
s47   0   1   1   1   1   1   0   1   1   1   0   0
s48   0   0   1   0   1   1   1   1   1   0   0   0
s49   0   1   0   1   1   1   1   1   1   0   0   1
s50   0   1   0   1   1   1   1   0   1   1   1   1
s51   0   1   0   1   1   1   1   0   1   1   1   1
s52   1   1   0   1   0   0   1   1   1   1   1   1
s53   1   1   0   0   1   1   1   1   1   1   1   1
s54   1   1   1   0   1   1   1   1   1   1   1   1
s55   1   1   1   0   1   1   1   1   1   1   1   1
s56   1   1   1   0   1   1   1   1   0   1   1   1
s57   0   0   1   1   1   0   0   1   1   1   1   1
s58   0   0   1   1   1   0   0   1   1   1   1   1
s59   0   0   1   1   1   0   0   1   1   1   1   1
s60   0   0   1   1   1   1   1   1   1   1   1   1
s61   0   0   0   1   1   1   1   0   1   1   0   1
s62   1   0   0   1   0   1   1   0   0   0   1   1
s63   1   1   1   0   0   1   1   1   1   1   1   1
s64   1   1   0   0   0   1   1   1   1   1   1   1
s65   1   1   1   1   1   0   1   0   1   0   1   1
s66   1   1   1   1   1   1   0   1   1   0   1   0
s67   1   0   0   1   1   0   1   0   1   1   1   1
s68   1   1   0   1   1   1   1   1   0   0   1   1
s69   1   1   0   1   1   0   0   1   0   0   1   1
s70   1   0   1   1   1   1   1   1   1   1   0   1
s71   1   1   1   1   1   1   0   1   1   1   1   0
> foo <- function(tt, net, par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
+ }
> foo(log(c(q01, q10, pSpec)))
Error in foo(log(c(q01, q10, pSpec))) (from #2) :
  argument "par" is missing, with no default
> foo(tt, net, log(c(q01, q10, pSpec)))
[1] 849.7291
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
+ }
> foo(log(c(q01, q10, pSpec)))
[1] 849.7291
> out <- optim(log(c(q01, q10, pSpec)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 849.729100
  Scaled convergence tolerance is 1.2662e-05
Stepsize computed as 0.094909
Exiting from Nelder Mead minimizer
    4 function evaluations used
> out
$par
[1] -0.9113506 -0.9490926 -0.4735361

$value
[1] 849.7291

$counts
function gradient
       4       NA

$convergence
[1] 0

$message
NULL

> pSpec <- runif(1, sProb[1], sProb[2])
> q01 <- runif(1, qProb[1], qProb[2])
> q10 <- runif(1, qProb[1], qProb[2])
> lambda <- runif(1, spProb[1], spProb[2])
> ntaxa <- round(runif(1, tProb[1], tProb[2]))
> tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
> tt

Phylogenetic tree with 54 tips and 53 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
> net
    s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 s19 s20 s21
s1   1  1  1  1  1  0  1  1  0   0   1   1   0   0   0   0   1   1   0   0   1
s2   1  1  1  1  1  0  1  1  0   0   1   1   0   0   0   0   1   1   0   0   1
s3   1  1  0  1  1  0  1  1  1   1   1   0   0   1   0   1   1   0   1   0   1
s4   1  1  1  0  1  0  0  0  1   1   1   1   1   1   0   1   1   0   1   1   1
s5   1  1  1  1  0  1  1  1  1   1   0   1   1   0   1   1   1   0   0   0   1
s6   0  0  0  0  1  0  1  1  1   1   1   1   1   1   1   1   1   0   0   0   1
s7   1  1  1  0  1  1  0  1  0   0   0   1   0   0   0   1   0   1   0   1   0
s8   1  1  1  0  1  1  1  0  0   0   0   1   0   0   0   1   0   1   0   1   0
s9   0  0  1  1  1  1  0  0  0   1   1   1   1   0   0   1   1   0   1   0   1
s10  0  0  1  1  1  1  0  0  1   0   1   1   1   0   0   1   1   0   1   0   1
s11  1  1  1  1  0  1  0  0  1   1   0   1   1   0   0   0   0   1   1   1   1
s12  1  1  0  1  1  1  1  1  1   1   1   0   1   0   1   1   1   0   0   1   1
s13  0  0  0  1  1  1  0  0  1   1   1   1   0   0   1   1   1   1   0   1   0
s14  0  0  1  1  0  1  0  0  0   0   0   0   0   0   0   1   1   0   0   0   0
s15  0  0  0  0  1  1  0  0  0   0   0   1   1   0   0   0   0   0   1   1   1
s16  0  0  1  1  1  1  1  1  1   1   0   1   1   1   0   0   0   1   1   1   1
s17  1  1  1  1  1  1  0  0  1   1   0   1   1   1   0   0   0   1   1   1   1
s18  1  1  0  0  0  0  1  1  0   0   1   0   1   0   0   1   1   0   1   1   1
s19  0  0  1  1  0  0  0  0  1   1   1   0   0   0   1   1   1   1   0   1   0
s20  0  0  0  1  0  0  1  1  0   0   1   1   1   0   1   1   1   1   1   0   0
s21  1  1  1  1  1  1  0  0  1   1   1   1   0   0   1   1   1   1   0   0   0
s22  0  0  1  1  0  1  1  1  1   1   0   1   0   0   1   1   1   0   0   0   1
s23  1  1  1  1  1  0  1  1  1   1   0   1   1   0   1   0   1   1   0   0   1
s24  1  1  1  1  1  1  1  1  1   1   0   1   1   0   1   1   0   1   1   0   0
s25  0  0  1  1  1  1  1  1  1   1   0   1   0   0   1   0   0   0   0   0   1
s26  1  1  1  0  1  1  0  0  0   1   0   0   1   1   1   1   1   0   1   1   1
s27  0  0  1  0  0  0  1  1  1   1   1   1   0   1   1   1   1   0   0   0   0
s28  0  0  1  0  0  1  0  0  1   1   0   1   0   0   0   0   0   0   0   0   1
s29  0  0  1  1  0  0  0  0  1   1   0   0   0   1   0   0   1   1   0   0   0
s30  0  0  0  0  0  1  0  0  1   1   0   1   0   0   0   1   0   0   0   1   1
s31  0  0  0  0  0  0  1  0  1   1   0   1   0   0   1   0   0   0   1   0   1
s32  0  0  0  0  1  1  0  0  0   0   1   1   0   0   1   0   0   0   1   1   0
s33  0  0  1  0  1  0  1  1  0   1   0   1   0   0   1   0   0   0   1   1   0
s34  1  1  1  1  1  1  0  0  0   0   0   1   1   1   0   1   1   0   1   1   1
s35  1  1  0  1  1  1  1  1  1   1   0   1   1   0   0   0   1   1   1   0   0
s36  1  1  1  0  1  1  1  1  1   0   0   0   0   1   1   1   0   1   0   1   0
s37  0  0  0  1  0  1  0  0  1   1   0   0   1   0   1   1   1   0   0   0   1
s38  1  1  0  0  0  0  1  1  1   1   0   0   1   1   1   1   1   1   0   0   0
s39  1  1  1  1  1  0  0  0  0   0   0   0   0   0   1   1   1   1   0   0   0
s40  1  1  1  0  1  1  0  0  0   0   1   0   1   0   0   1   1   1   0   0   1
s41  0  0  1  1  0  0  1  1  1   1   0   1   1   0   0   0   0   1   0   0   1
s42  0  0  0  0  0  0  1  0  1   1   1   1   1   1   1   1   1   1   1   1   0
s43  1  1  1  1  0  0  1  1  1   1   1   1   1   1   0   0   1   0   0   0   0
s44  1  1  1  1  0  0  1  1  1   1   1   0   0   1   0   1   1   1   0   0   1
s45  0  0  0  0  1  1  1  1  0   0   1   0   0   0   0   1   1   0   0   0   0
s46  1  1  1  1  1  0  1  1  1   0   1   0   0   0   0   1   1   1   0   0   0
s47  1  1  1  1  1  0  0  0  1   1   0   1   0   1   0   1   1   0   0   0   0
s48  1  1  0  1  0  1  0  0  1   1   0   1   0   0   0   1   0   0   0   0   0
s49  0  0  1  1  0  1  0  1  1   1   0   1   0   0   0   1   1   0   0   1   1
s50  1  1  0  0  0  0  1  1  1   1   0   0   0   1   1   1   0   1   1   0   1
s51  1  1  1  0  1  1  0  0  1   1   1   0   0   1   1   1   1   1   1   1   0
s52  1  1  1  0  0  1  0  0  1   0   1   1   1   1   0   1   1   1   1   1   0
s53  0  0  0  0  1  1  0  0  0   1   0   0   0   1   0   1   0   1   1   1   1
s54  0  0  1  0  1  1  0  0  0   0   0   0   0   0   0   1   0   1   0   1   1
    s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 s36 s37 s38 s39 s40
s1    0   1   1   0   1   0   0   0   0   0   0   0   1   1   1   0   1   1   1
s2    0   1   1   0   1   0   0   0   0   0   0   0   1   1   1   0   1   1   1
s3    1   1   1   1   1   1   1   1   0   0   0   1   1   0   1   0   0   1   1
s4    1   1   1   1   0   0   0   1   0   0   0   0   1   1   0   1   0   1   0
s5    0   1   1   1   1   0   0   0   0   0   1   1   1   1   1   0   0   1   1
s6    1   0   1   1   1   0   1   0   1   0   1   0   1   1   1   1   0   0   1
s7    1   1   1   1   0   1   0   0   0   1   0   1   0   1   1   0   1   0   0
s8    1   1   1   1   0   1   0   0   0   0   0   1   0   1   1   0   1   0   0
s9    1   1   1   1   0   1   1   1   1   1   0   0   0   1   1   1   1   0   0
s10   1   1   1   1   1   1   1   1   1   1   0   1   0   1   0   1   1   0   0
s11   0   0   0   0   0   1   0   0   0   0   1   0   0   0   0   0   0   0   1
s12   1   1   1   1   0   1   1   0   1   1   1   1   1   1   0   0   0   0   0
s13   0   1   1   0   1   0   0   0   0   0   0   0   1   1   0   1   1   0   1
s14   0   0   0   0   1   1   0   1   0   0   0   0   1   0   1   0   1   0   0
s15   1   1   1   1   1   1   0   0   0   1   1   1   0   0   1   1   1   1   0
s16   1   0   1   0   1   1   0   0   1   0   0   0   1   0   1   1   1   1   1
s17   1   1   0   0   1   1   0   1   0   0   0   0   1   1   0   1   1   1   1
s18   0   1   1   0   0   0   0   1   0   0   0   0   0   1   1   0   1   1   1
s19   0   0   1   0   1   0   0   0   0   1   1   1   1   1   0   0   0   0   0
s20   0   0   0   0   1   0   0   0   1   0   1   1   1   0   1   0   0   0   0
s21   1   1   0   1   1   0   1   0   1   1   0   0   1   0   0   1   0   0   1
s22   0   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   1   1
s23   1   0   1   1   1   1   1   0   0   0   0   0   1   1   1   0   0   0   1
s24   1   1   0   1   1   1   0   0   0   0   0   1   1   0   1   0   0   0   1
s25   1   1   1   0   0   1   1   1   1   1   1   1   0   0   1   1   1   0   0
s26   1   1   1   0   0   1   0   0   1   1   1   1   0   1   1   1   1   0   1
s27   0   1   1   1   1   0   1   0   1   1   1   1   1   0   0   0   0   1   0
s28   0   1   0   1   0   1   0   0   1   1   0   0   1   0   0   0   0   0   0
s29   0   0   0   1   0   0   0   0   1   1   1   1   0   1   0   1   0   1   0
s30   0   0   0   1   1   1   1   1   0   1   1   1   0   1   0   1   1   1   1
s31   0   0   0   1   1   1   1   1   1   0   1   1   0   1   0   1   0   1   1
s32   0   0   0   1   1   1   0   1   1   1   0   1   1   1   1   1   0   0   1
s33   0   0   1   1   1   1   0   1   1   1   1   0   1   0   0   1   0   0   1
s34   0   1   1   0   0   1   1   0   0   0   1   1   0   1   0   0   1   0   1
s35   0   1   0   0   1   0   0   1   1   1   1   0   1   0   1   1   1   0   1
s36   0   1   1   1   1   0   0   0   0   0   1   0   0   1   0   0   0   1   0
s37   0   0   0   1   1   0   0   1   1   1   1   1   0   1   0   0   1   1   0
s38   0   0   0   1   1   0   0   0   1   0   0   0   1   1   0   1   0   1   1
s39   1   0   0   0   0   1   0   1   1   1   0   0   0   0   1   1   1   0   0
s40   1   1   1   0   1   0   0   0   1   1   1   1   1   1   0   0   1   0   0
s41   1   0   0   0   0   1   1   1   0   1   1   1   1   0   0   0   1   1   1
s42   1   1   1   0   1   1   1   1   1   1   1   1   0   0   1   1   1   0   1
s43   0   0   0   1   0   1   1   0   0   0   0   0   1   1   0   1   0   0   1
s44   0   0   1   1   0   1   1   1   1   1   0   0   0   1   0   1   0   1   1
s45   0   1   0   1   1   0   1   0   1   0   1   1   1   0   1   0   1   0   0
s46   1   1   1   0   0   1   1   1   1   1   1   1   1   1   0   1   0   1   1
s47   1   0   1   1   0   1   1   0   1   1   1   1   0   1   1   0   0   1   0
s48   1   1   1   1   0   1   1   1   1   1   1   1   1   1   0   0   0   0   0
s49   1   1   1   0   1   1   1   1   1   1   1   1   0   0   1   0   0   1   0
s50   0   0   0   0   0   1   1   1   1   1   1   1   0   0   1   0   1   1   0
s51   1   0   0   0   1   0   0   0   1   0   0   0   0   0   0   0   0   1   1
s52   0   0   0   1   0   0   1   0   1   0   0   0   1   1   0   1   1   0   1
s53   0   1   1   0   0   0   0   0   1   1   1   1   0   0   1   0   1   0   1
s54   1   1   1   0   0   1   1   0   1   1   1   1   0   0   1   0   1   1   1
    s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 s54
s1    0   0   1   1   0   1   1   1   0   1   1   1   0   0
s2    0   0   1   1   0   1   1   1   0   1   1   1   0   0
s3    1   0   1   1   0   1   1   0   1   0   1   1   0   1
s4    1   0   1   1   0   1   1   1   1   0   0   0   0   0
s5    0   0   0   0   1   1   1   0   0   0   1   0   1   1
s6    0   0   0   0   1   0   0   1   1   0   1   1   1   1
s7    1   1   1   1   1   1   0   0   0   1   0   0   0   0
s8    1   0   1   1   1   1   0   0   1   1   0   0   0   0
s9    1   1   1   1   0   1   1   1   1   1   1   1   0   0
s10   1   1   1   1   0   0   1   1   1   1   1   0   1   0
s11   0   1   1   1   1   1   0   0   0   0   1   1   0   0
s12   1   1   1   0   0   0   1   1   1   0   0   1   0   0
s13   1   1   1   0   0   0   0   0   0   0   0   1   0   0
s14   0   1   1   1   0   0   1   0   0   1   1   1   1   0
s15   0   1   0   0   0   0   0   0   0   1   1   0   0   0
s16   0   1   0   1   1   1   1   1   1   1   1   1   1   1
s17   0   1   1   1   1   1   1   0   1   0   1   1   0   0
s18   1   1   0   1   0   1   0   0   0   1   1   1   1   1
s19   0   1   0   0   0   0   0   0   0   1   1   1   1   0
s20   0   1   0   0   0   0   0   0   1   0   1   1   1   1
s21   1   0   0   1   0   0   0   0   1   1   0   0   1   1
s22   1   1   0   0   0   1   1   1   1   0   1   0   0   1
s23   0   1   0   0   1   1   0   1   1   0   0   0   1   1
s24   0   1   0   1   0   1   1   1   1   0   0   0   1   1
s25   0   0   1   1   1   0   1   1   0   0   0   1   0   0
s26   0   1   0   0   1   0   0   0   1   0   1   0   0   0
s27   1   1   1   1   0   1   1   1   1   1   0   0   0   1
s28   1   1   1   1   1   1   1   1   1   1   0   1   0   1
s29   1   1   0   1   0   1   0   1   1   1   0   0   0   0
s30   0   1   0   1   1   1   1   1   1   1   1   1   1   1
s31   1   1   0   1   0   1   1   1   1   1   0   0   1   1
s32   1   1   0   0   1   1   1   1   1   1   0   0   1   1
s33   1   1   0   0   1   1   1   1   1   1   0   0   1   1
s34   1   0   1   0   1   1   0   1   0   0   0   1   0   0
s35   0   0   1   1   0   1   1   1   0   0   0   1   0   0
s36   0   1   0   0   1   0   1   0   1   1   0   0   1   1
s37   0   1   1   1   0   1   0   0   0   0   0   1   0   0
s38   1   1   0   0   1   0   0   0   0   1   0   1   1   1
s39   1   0   0   1   0   1   1   0   1   1   1   0   0   1
s40   1   1   1   1   0   1   0   0   0   0   1   1   1   1
s41   0   1   1   1   0   1   0   1   0   1   0   1   0   0
s42   1   0   0   0   0   0   0   0   1   0   0   0   1   1
s43   1   0   0   0   0   0   0   1   0   1   0   0   0   0
s44   1   0   0   0   1   0   0   1   1   1   0   1   0   0
s45   0   0   0   1   0   0   0   0   0   0   1   0   0   1
s46   1   0   0   0   0   0   1   0   1   0   0   0   0   0
s47   0   0   0   0   0   1   0   0   0   0   0   0   0   0
s48   1   0   1   1   0   0   0   0   0   0   1   1   1   1
s49   0   1   0   1   0   1   0   0   0   1   1   1   1   1
s50   1   0   1   1   0   0   0   0   1   0   1   1   1   1
s51   0   0   0   0   1   0   0   1   1   1   0   0   1   1
s52   1   0   0   1   0   0   0   1   1   1   0   0   0   0
s53   0   1   0   0   0   0   0   1   1   1   1   0   0   1
s54   0   1   0   0   1   0   0   1   1   1   1   0   1   0
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
+ }
> foo(log(c(q01, q10, pSpec)))
[1] 845.4045
> out <- optim(log(c(q01, q10, pSpec)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 845.404499
  Scaled convergence tolerance is 1.25975e-05
Stepsize computed as 0.064294
Exiting from Nelder Mead minimizer
    4 function evaluations used
> out
$par
[1] -0.6285747 -0.5659491  0.6429440

$value
[1] 845.4045

$counts
function gradient
       4       NA

$convergence
[1] 0

$message
NULL

> exp(out$par)
[1] 0.5333514 0.5678210 1.9020724
> par[1]
Error in par[1] : object of type 'closure' is not subsettable
> c(q01, q10, pSpec)
[1] 0.5333514 0.5678210 1.9020724
> pSpec
[1] 1.902072
> pSpec <- runif(1, sProb[1], sProb[2])
> q01 <- runif(1, qProb[1], qProb[2])
> q10 <- runif(1, qProb[1], qProb[2])
> lambda <- runif(1, spProb[1], spProb[2])
> ntaxa <- round(runif(1, tProb[1], tProb[2]))
>
>
> tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
>
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
>
>
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
+ }
> foo(log(c(q01, q10, pSpec)))
[1] 0.1769683
> out <- optim(log(c(q01, q10, pSpec)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 0.176968
  Scaled convergence tolerance is 2.63703e-09
Stepsize computed as 0.171318
Exiting from Nelder Mead minimizer
    4 function evaluations used
> c(q01, q10, pSpec)
[1] 0.1802915 0.7163171 1.3229361
> exp(out$par)
[1] 0.1802915 0.7163171 1.3229361
> all.equal(c(q01, q10, pSpec), exp(out$par))
[1] TRUE
> ntaxa <- 4
> tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
> tt

Phylogenetic tree with 4 tips and 3 internal nodes.

Tip labels:
[1] "s1" "s2" "s3" "s4"

Rooted; includes branch lengths.
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
+ }
> foo(log(c(q01, q10, pSpec)))
[1] 1.865939
> out <- optim(log(c(q01, q10, pSpec)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 1.865939
  Scaled convergence tolerance is 2.78047e-08
Stepsize computed as 0.171318
Exiting from Nelder Mead minimizer
    4 function evaluations used
> c(q01, q10, pSpec)
[1] 0.1802915 0.7163171 1.3229361
> exp(out$par)
[1] 0.1802915 0.7163171 1.3229361
> out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 1.865939
  Scaled convergence tolerance is 2.78047e-08
Stepsize computed as 0.069315
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.5 0.5 0.5
> out
$par
[1] -0.6931472 -0.6931472 -0.6931472

$value
[1] 1.865939

$counts
function gradient
       4       NA

$convergence
[1] 0

$message
NULL

> tt <- drop.tip(tt, "s4")
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
+ }
> foo(log(c(q01, q10, pSpec)))
[1] 0.8429964
> out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 0.842996
  Scaled convergence tolerance is 1.25616e-08
Stepsize computed as 0.069315
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.5 0.5 0.5
> lambda
[1] 0.5847414
> pSpec <- runif(1, sProb[1], sProb[2])
> q01 <- runif(1, qProb[1], qProb[2])
> q10 <- runif(1, qProb[1], qProb[2])
> lambda <- runif(1, spProb[1], spProb[2])
> ntaxa <- round(runif(1, tProb[1], tProb[2]))
> tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
> tt <- drop.tip(tt, paste0("s", ntaxa))
> tt

Phylogenetic tree with 72 tips and 71 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> ntaxa
[1] 73
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
+ }
> foo(log(c(q01, q10, pSpec)))
[1] 713.0776
> out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 713.077632
  Scaled convergence tolerance is 1.06257e-05
Stepsize computed as 0.069315
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.5 0.5 0.5
> fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb))
+ foo(log(c(0.1, 0.1, 0.5)))
Error: unexpected symbol in:
"fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb))
foo"
> out<-optim(log(c(0.5, 0.5, 0.5)), foo, control=list(trace=6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 713.077632
  Scaled convergence tolerance is 1.06257e-05
Stepsize computed as 0.069315
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.5 0.5 0.5
> fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
[1] 713.0776
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(par[1], par[2], par[3]))
+ }
> foo(log(c(q01, q10, pSpec)))
Error in if (logx == -Inf) return(logy) else max(logx, logy) + log1p(exp(-abs(logx -  (from logspace_add.R#3) :
  missing value where TRUE/FALSE needed
In addition: Warning messages:
1: In log(p[i, ]) : NaNs produced
2: In log(p[i, ]) : NaNs produced
> pSpec <- runif(1, sProb[1], sProb[2])
> q01 <- runif(1, qProb[1], qProb[2])
> q10 <- runif(1, qProb[1], qProb[2])
> lambda <- runif(1, spProb[1], spProb[2])
> ntaxa <- round(runif(1, tProb[1], tProb[2]))
> tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
> tt <- drop.tip(tt, paste0("s", ntaxa))
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(par[1], par[2], par[3]))
+ }
> foo(log(c(q01, q10, pSpec)))
Error in if (logx == -Inf) return(logy) else max(logx, logy) + log1p(exp(-abs(logx -  (from logspace_add.R#3) :
  missing value where TRUE/FALSE needed
In addition: Warning messages:
1: In log(p[i, ]) : NaNs produced
2: In log(p[i, ]) : NaNs produced
> tt

Phylogenetic tree with 97 tips and 96 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> ntaxa
[1] 98
> phy<-read.tree(text="(((a:1.0,b:1.0):2.0,(c:0.5,d:0.5):2.5):0.5,e:3.5);")
> network<-rbind(c(0, 1, 0, 0, 1),
+                c(1, 0, 0, 0, 1),
+                c(0, 0, 0, 0, 0),
+                c(0, 0, 0, 0, 1),
+                c(1, 1, 0, 1, 0))
> rownames(network)<-c("a","b","c","d","e")
> colnames(network)<-c("a","b","c","d","e")
> q01<-0.1
> q10<-0.1
> pSpec<-0.5
> fitPhyloNetwork(phy, network, pars = c(q01, q10, pSpec))
[1] 6.336294
> foo<-function(x) {
+   q01<-exp(x[1])
+   q10<-exp(x[2])
+   pSpec<-exp(x[3])
+   fitPhyloNetwork(phy, network, c(q01, q10, pSpec))
+ }
> foo(log(c(0.1, 0.1, 0.7)))
[1] 6.599091
> out<-optim(log(c(0.5, 0.5, 0.5)), foo, control=list(trace=6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 6.382582
  Scaled convergence tolerance is 9.51079e-08
Stepsize computed as 0.069315
BUILD              4 6.419065 6.382582
EXTENSION          6 6.405356 6.375414
REFLECTION         8 6.389865 6.366040
EXTENSION         10 6.382582 6.345222
HI-REDUCTION      12 6.375414 6.345222
EXTENSION         14 6.366040 6.320911
EXTENSION         16 6.363070 6.289191
EXTENSION         18 6.345222 6.256692
EXTENSION         20 6.320911 6.186630
EXTENSION         22 6.289191 6.143265
LO-REDUCTION      24 6.256692 6.143265
REFLECTION        26 6.186630 6.120582
LO-REDUCTION      28 6.159200 6.120582
LO-REDUCTION      30 6.143265 6.120582
HI-REDUCTION      32 6.140743 6.120582
EXTENSION         34 6.131710 6.112633
LO-REDUCTION      36 6.129481 6.112633
LO-REDUCTION      38 6.120582 6.112477
HI-REDUCTION      40 6.114287 6.112477
HI-REDUCTION      42 6.113982 6.112214
HI-REDUCTION      44 6.112633 6.112214
HI-REDUCTION      46 6.112477 6.111833
HI-REDUCTION      48 6.112224 6.111833
LO-REDUCTION      50 6.112214 6.111725
HI-REDUCTION      52 6.111836 6.111725
REFLECTION        54 6.111833 6.111677
HI-REDUCTION      56 6.111775 6.111677
HI-REDUCTION      58 6.111725 6.111677
HI-REDUCTION      60 6.111690 6.111670
HI-REDUCTION      62 6.111687 6.111665
LO-REDUCTION      64 6.111677 6.111660
HI-REDUCTION      66 6.111670 6.111659
HI-REDUCTION      68 6.111665 6.111658
REFLECTION        70 6.111660 6.111656
REFLECTION        72 6.111659 6.111655
LO-REDUCTION      74 6.111658 6.111654
HI-REDUCTION      76 6.111656 6.111654
HI-REDUCTION      78 6.111655 6.111653
HI-REDUCTION      80 6.111654 6.111653
LO-REDUCTION      82 6.111654 6.111653
HI-REDUCTION      84 6.111654 6.111653
LO-REDUCTION      86 6.111653 6.111653
HI-REDUCTION      88 6.111653 6.111653
Exiting from Nelder Mead minimizer
    90 function evaluations used
> exp(out$par)
[1] 0.2580852 0.2193863 0.3479643
> pSpec
[1] 0.5
> tt

Phylogenetic tree with 97 tips and 96 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> phy<-read.tree(text="(((a:1.0,b:1.0):2.0,(c:0.5,d:0.5):2.5):0.5,e:3.5);")
> tt<-sim.bdtree(b=1, d=0, stop="taxa", n = 60)
> tt<-drop.tip(tt, "60")
> tt

Phylogenetic tree with 60 tips and 59 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> tt<-sim.bdtree(b=1, d=0, stop="taxa", n = 60)
> tt

Phylogenetic tree with 60 tips and 59 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> tt<-drop.tip(tt, "s60")
> xx<-simPhyloNetwork(tt, qRate=0.1, sProb=0.5)
> fitPhyloNetwork(tt, xx, c(0.1, 0.1, 0.5))
[1] 302.1457
> foo<-function(x) {
+   q01<-exp(x[1])
+   q10<-exp(x[2])
+   pSpec<-exp(x[3])
+   fitPhyloNetwork(tt, xx, c(0.1, 0.1, 0.5))
+ }
> foo(log(c(0.1, 0.1, 0.5)))
[1] 302.1457
> out<-optim(log(c(0.5, 0.5, 0.5)), foo, control=list(trace=6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 302.145675
  Scaled convergence tolerance is 4.50232e-06
Stepsize computed as 0.069315
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.5 0.5 0.5
> phy<-read.tree(text="(((a:1.0,b:1.0):2.0,(c:0.5,d:0.5):2.5):0.5,e:3.5);")
> network<-rbind(c(0, 1, 0, 0, 1),
+                c(1, 0, 0, 0, 1),
+                c(0, 0, 0, 0, 0),
+                c(0, 0, 0, 0, 1),
+                c(1, 1, 0, 1, 0))
> rownames(network)<-c("a","b","c","d","e")
> colnames(network)<-c("a","b","c","d","e")
>
> q01<-0.1
> q10<-0.1
> pSpec<-0.5
>
> fitPhyloNetwork(phy, network, pars = c(q01, q10, pSpec))
[1] 6.336294
>
> foo<-function(x) {
+   q01<-exp(x[1])
+   q10<-exp(x[2])
+   pSpec<-exp(x[3])
+   fitPhyloNetwork(phy, network, c(q01, q10, pSpec))
+ }
>
> foo(log(c(0.1, 0.1, 0.7)))
[1] 6.599091
>
> out<-optim(log(c(0.5, 0.5, 0.5)), foo, control=list(trace=6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 6.382582
  Scaled convergence tolerance is 9.51079e-08
Stepsize computed as 0.069315
BUILD              4 6.419065 6.382582
EXTENSION          6 6.405356 6.375414
REFLECTION         8 6.389865 6.366040
EXTENSION         10 6.382582 6.345222
HI-REDUCTION      12 6.375414 6.345222
EXTENSION         14 6.366040 6.320911
EXTENSION         16 6.363070 6.289191
EXTENSION         18 6.345222 6.256692
EXTENSION         20 6.320911 6.186630
EXTENSION         22 6.289191 6.143265
LO-REDUCTION      24 6.256692 6.143265
REFLECTION        26 6.186630 6.120582
LO-REDUCTION      28 6.159200 6.120582
LO-REDUCTION      30 6.143265 6.120582
HI-REDUCTION      32 6.140743 6.120582
EXTENSION         34 6.131710 6.112633
LO-REDUCTION      36 6.129481 6.112633
LO-REDUCTION      38 6.120582 6.112477
HI-REDUCTION      40 6.114287 6.112477
HI-REDUCTION      42 6.113982 6.112214
HI-REDUCTION      44 6.112633 6.112214
HI-REDUCTION      46 6.112477 6.111833
HI-REDUCTION      48 6.112224 6.111833
LO-REDUCTION      50 6.112214 6.111725
HI-REDUCTION      52 6.111836 6.111725
REFLECTION        54 6.111833 6.111677
HI-REDUCTION      56 6.111775 6.111677
HI-REDUCTION      58 6.111725 6.111677
HI-REDUCTION      60 6.111690 6.111670
HI-REDUCTION      62 6.111687 6.111665
LO-REDUCTION      64 6.111677 6.111660
HI-REDUCTION      66 6.111670 6.111659
HI-REDUCTION      68 6.111665 6.111658
REFLECTION        70 6.111660 6.111656
REFLECTION        72 6.111659 6.111655
LO-REDUCTION      74 6.111658 6.111654
HI-REDUCTION      76 6.111656 6.111654
HI-REDUCTION      78 6.111655 6.111653
HI-REDUCTION      80 6.111654 6.111653
LO-REDUCTION      82 6.111654 6.111653
HI-REDUCTION      84 6.111654 6.111653
LO-REDUCTION      86 6.111653 6.111653
HI-REDUCTION      88 6.111653 6.111653
Exiting from Nelder Mead minimizer
    90 function evaluations used
> exp(out$par)
[1] 0.2580852 0.2193863 0.3479643
> tt<-sim.bdtree(b=1, d=0, stop="taxa", n = 60)
> tt<-drop.tip(tt, "s60")
> xx<-simPhyloNetwork(tt, qRate=0.1, sProb=0.5)
> tt

Phylogenetic tree with 59 tips and 58 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> xx
    s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 s19 s20 s21
s1   0  1  0  0  0  0  0  0  0   0   1   1   1   1   1   1   1   1   1   1   1
s2   1  0  0  0  0  0  0  0  0   0   0   0   1   1   0   0   1   1   0   1   1
s3   0  0  0  1  1  0  0  0  0   0   1   1   1   1   0   0   1   1   1   1   1
s4   0  0  1  0  1  0  0  0  0   0   1   0   1   1   0   0   1   1   1   1   1
s5   0  0  1  1  0  0  0  0  0   0   1   1   1   1   0   0   1   1   1   1   1
s6   0  0  0  0  0  0  0  0  1   1   1   1   1   1   1   1   1   0   1   1   1
s7   0  0  0  0  0  0  0  0  1   1   1   1   1   1   1   1   1   0   1   1   1
s8   0  0  0  0  0  0  0  0  1   1   1   1   1   1   1   1   1   1   1   1   1
s9   0  0  0  0  0  1  1  1  0   1   1   1   1   1   1   1   1   1   1   1   1
s10  0  0  0  0  0  1  1  1  1   0   1   1   0   1   1   1   0   0   1   1   1
s11  1  0  1  1  1  1  1  1  1   1   0   1   0   0   0   0   0   0   0   0   0
s12  1  0  1  0  1  1  1  1  1   1   1   0   0   0   0   0   0   0   0   0   0
s13  1  1  1  1  1  1  1  1  1   0   0   0   0   1   0   0   0   0   0   0   0
s14  1  1  1  1  1  1  1  1  1   1   0   0   1   0   1   1   0   0   0   0   0
s15  1  0  0  0  0  1  1  1  1   1   0   0   0   1   0   1   1   1   0   1   0
s16  1  0  0  0  0  1  1  1  1   1   0   0   0   1   1   0   1   1   0   0   0
s17  1  1  1  1  1  1  1  1  1   0   0   0   0   0   1   1   0   0   0   0   0
s18  1  1  1  1  1  0  0  1  1   0   0   0   0   0   1   1   0   0   0   0   0
s19  1  0  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   0   0
s20  1  1  1  1  1  1  1  1  1   1   0   0   0   0   1   0   0   0   0   0   1
s21  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   1   0
s22  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   1   1
s23  1  1  1  1  1  0  0  1  1   1   0   0   0   0   1   1   0   1   0   0   1
s24  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   0   0
s25  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   0   0
s26  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   0   0
s27  1  0  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   0   0
s28  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   0   0
s29  1  1  1  1  1  1  1  0  1   1   0   0   0   0   0   0   0   0   0   0   0
s30  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   0   0
s31  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   0   0   0   0   0
s32  0  0  0  0  0  0  0  0  1   1   0   0   0   0   1   0   0   0   0   0   0
s33  0  0  0  0  0  1  1  1  1   1   0   0   0   0   0   0   0   0   1   1   1
s34  0  0  0  0  0  0  0  0  1   0   0   0   0   0   1   1   0   0   1   1   1
s35  0  0  0  0  0  0  0  0  1   0   0   0   0   0   1   1   0   0   1   1   1
s36  0  0  0  0  0  0  0  0  1   0   0   0   0   0   1   1   0   0   1   1   1
s37  0  0  0  0  0  0  0  0  0   0   0   0   0   0   1   1   0   0   1   1   1
s38  0  0  0  0  0  0  0  0  0   1   0   0   0   0   1   1   0   0   1   1   1
s39  0  0  1  0  0  0  0  0  0   1   0   0   0   0   1   1   0   0   1   1   1
s40  1  0  0  0  0  0  0  0  0   1   0   0   0   0   1   0   0   0   1   1   1
s41  1  0  0  0  0  0  0  0  0   1   0   0   0   0   1   0   0   0   1   1   1
s42  1  0  0  0  0  0  0  0  0   0   0   0   0   0   1   1   0   0   1   1   1
s43  0  1  0  0  0  0  0  0  0   0   1   1   0   0   0   0   0   0   1   1   1
s44  0  0  0  0  0  0  0  0  0   0   1   1   0   1   0   0   0   0   1   1   1
s45  1  1  1  1  1  1  1  1  1   1   0   0   0   1   0   0   1   1   0   0   0
s46  1  1  1  1  1  1  1  1  1   1   0   0   1   0   0   0   1   1   0   0   0
s47  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   1   1   0   0   0
s48  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   1   1   0   0   0
s49  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   1   1   0   0   0
s50  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   1   1   0   0   0
s51  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   1   1   0   0   0
s52  1  1  1  1  1  1  1  1  1   1   0   0   0   0   0   0   1   1   0   0   0
s53  1  0  0  0  0  0  0  0  1   0   0   0   0   0   0   0   0   0   1   0   0
s54  0  0  0  0  0  0  0  0  0   0   0   0   0   1   0   0   0   0   0   0   0
s55  0  0  0  0  0  0  0  0  0   0   0   0   0   0   0   0   0   0   0   0   0
s56  0  0  0  0  0  0  0  0  0   0   1   1   0   0   0   0   0   0   0   0   0
s57  1  1  1  1  1  0  0  0  0   0   1   1   1   1   0   0   0   0   0   0   0
s58  1  1  1  1  1  0  0  0  0   0   1   1   0   0   0   0   0   0   0   0   0
s59  0  0  0  0  0  0  0  0  0   0   0   0   1   0   0   1   1   1   0   0   0
    s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 s36 s37 s38 s39 s40
s1    1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   1
s2    1   1   1   1   1   0   1   1   1   1   0   0   0   0   0   0   0   0   0
s3    1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   1   0
s4    1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   0
s5    1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0   0   0
s6    1   0   1   1   1   1   1   1   1   1   0   1   0   0   0   0   0   0   0
s7    1   0   1   1   1   1   1   1   1   1   0   1   0   0   0   0   0   0   0
s8    1   1   1   1   1   1   1   0   1   1   0   1   0   0   0   0   0   0   0
s9    1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0   0   0   0
s10   1   1   1   1   1   1   1   1   1   1   1   1   0   0   0   0   1   1   1
s11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s15   0   1   0   0   0   0   0   0   0   0   1   0   1   1   1   1   1   1   1
s16   0   1   0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   0
s17   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s18   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s19   0   0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1
s20   1   0   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1
s21   1   1   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1
s22   0   1   0   0   0   0   0   0   0   0   0   1   1   1   1   1   1   1   1
s23   1   0   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
s24   0   1   0   1   1   0   0   1   1   1   1   1   1   1   1   1   1   1   1
s25   0   1   1   0   1   0   0   1   1   1   1   1   1   1   1   1   1   1   1
s26   0   1   1   1   0   0   0   1   1   1   1   1   1   1   1   1   1   1   1
s27   0   1   0   0   0   0   1   0   0   0   1   1   1   1   1   1   1   1   1
s28   0   1   0   0   0   1   0   0   0   0   1   1   1   1   1   1   1   1   1
s29   0   1   1   1   1   0   0   0   1   1   1   1   1   1   1   1   1   1   1
s30   0   1   1   1   1   0   0   1   0   1   1   1   1   1   1   1   1   1   1
s31   0   1   1   1   1   0   0   1   1   0   1   1   1   1   1   1   1   1   1
s32   0   1   1   1   1   1   1   1   1   1   0   1   1   1   1   1   1   1   1
s33   1   1   1   1   1   1   1   1   1   1   1   0   1   1   1   1   1   1   1
s34   1   1   1   1   1   1   1   1   1   1   1   1   0   1   0   0   1   1   1
s35   1   1   1   1   1   1   1   1   1   1   1   1   1   0   0   0   1   1   1
s36   1   1   1   1   1   1   1   1   1   1   1   1   0   0   0   0   1   1   1
s37   1   1   1   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0
s38   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0   0   1   1
s39   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0   1   0   0
s40   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0   1   0   0
s41   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   0   1   0   1
s42   1   1   1   1   1   1   1   1   1   1   1   1   0   0   0   0   1   1   1
s43   1   1   1   1   1   1   1   1   1   1   0   1   1   1   1   1   1   1   1
s44   1   1   1   1   0   1   1   1   1   1   1   1   1   1   1   1   1   1   1
s45   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   1   0   0
s46   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0
s47   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0
s48   0   0   0   0   0   1   0   1   0   0   0   0   0   0   0   0   0   0   0
s49   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0
s50   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0
s51   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0
s52   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0
s53   0   0   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s54   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s55   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0
s56   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0
s57   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s58   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s59   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
    s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 s54 s55 s56 s57 s58 s59
s1    1   1   0   0   1   1   1   1   1   1   1   1   1   0   0   0   1   1   0
s2    0   0   1   0   1   1   1   1   1   1   1   1   0   0   0   0   1   1   0
s3    0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   1   1   0
s4    0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   1   1   0
s5    0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   1   1   0
s6    0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0
s7    0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0
s8    0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0
s9    0   0   0   0   1   1   1   1   1   1   1   1   1   0   0   0   0   0   0
s10   1   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   0   0   0
s11   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   1   1   1   0
s12   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   1   1   1   0
s13   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   1   0   1
s14   0   0   0   1   1   0   0   0   0   0   0   0   0   1   0   0   1   0   0
s15   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s16   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1
s17   0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   0   0   1
s18   0   0   0   0   1   1   1   1   1   1   1   1   0   0   0   0   0   0   1
s19   1   1   1   1   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0
s20   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s21   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s22   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s23   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s24   1   1   1   1   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0
s25   1   1   1   1   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0
s26   1   1   1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   1   0
s27   1   1   1   1   1   1   1   1   1   1   1   1   0   0   1   1   0   0   0
s28   1   1   1   1   0   1   1   0   1   1   1   1   0   0   1   0   0   0   0
s29   1   1   1   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0
s30   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s31   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s32   1   1   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s33   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s34   1   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s35   1   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s36   1   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s37   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s38   1   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s39   0   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s40   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s41   0   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s42   1   0   1   1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0
s43   1   1   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s44   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
s45   0   0   0   0   0   1   1   1   1   1   1   1   0   1   1   0   1   0   0
s46   0   0   0   0   1   0   1   1   1   1   1   1   0   0   1   0   0   0   0
s47   0   0   0   0   1   1   0   1   0   0   0   0   0   1   1   0   0   0   0
s48   0   0   0   0   1   1   1   0   0   0   0   0   0   1   1   0   0   0   0
s49   0   0   0   0   1   1   0   0   0   1   1   0   0   1   1   0   1   0   0
s50   0   0   0   0   1   1   0   0   1   0   1   0   0   1   1   0   1   0   0
s51   0   0   0   0   1   1   0   0   1   1   0   0   0   1   1   0   0   0   0
s52   0   0   0   0   1   1   0   0   0   0   0   0   0   1   1   0   0   0   0
s53   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0   0   0
s54   0   1   0   0   1   0   1   1   1   1   1   1   1   0   1   0   0   0   0
s55   0   0   0   0   1   1   1   1   1   1   1   1   1   1   0   0   0   0   0
s56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   1
s57   0   0   0   0   1   0   0   0   1   1   0   0   0   0   0   1   0   1   0
s58   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1   0   0
s59   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0
> fitPhyloNetwork(tt, xx, c(0.1, 0.1, 0.5))
[1] 327.6095
> fitPhyloNetwork(tt, xx, c(0.1, 0.1, 0.7))
[1] 325.8817
> foo<-function(x) {
+   q01<-exp(x[1])
+   q10<-exp(x[2])
+   pSpec<-exp(x[3])
+   fitPhyloNetwork(tt, xx, c(0.1, 0.1, 0.5))
+ }
> foo(log(c(0.1, 0.1, 0.5)))
[1] 327.6095
> out<-optim(log(c(0.5, 0.5, 0.5)), foo, control=list(trace=6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 327.609484
  Scaled convergence tolerance is 4.88176e-06
Stepsize computed as 0.069315
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.5 0.5 0.5
> out<-optim(log(c(0.1, 0.1, 0.5)), foo, control=list(trace=6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 327.609484
  Scaled convergence tolerance is 4.88176e-06
Stepsize computed as 0.230259
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.1 0.1 0.5
> rm(list = ls())
> tt<-sim.bdtree(b=1, d=0, stop="taxa", n = 60)
> tt

Phylogenetic tree with 60 tips and 59 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> tt<-drop.tip(tt, "60")
> tt

Phylogenetic tree with 60 tips and 59 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> xx<-simPhyloNetwork(tt, qRate=0.1, sProb=0.5)
Error: could not find function "simPhyloNetwork"
> lapply(paste0("./R/",as.list(list.files("./R/"))), source)
[[1]]
[[1]]$value
function (Q)
{
    tmp <- eigen(Q, symmetric = FALSE)
    P1 <- tmp$vectors %*% diag(exp(tmp$values)) %*% solve(tmp$vectors)
    return(P1)
}

[[1]]$visible
[1] FALSE


[[2]]
[[2]]$value
function (tip.like, bl, q)
{
    nb.states <- length(tip.like)
    r <- rep(0, nb.states)
    p <- MatrixExp.eig(q * bl)
    for (i in 1:nb.states) r[i] <- logspace_sum(log(p[i, ]) +
        tip.like)
    return(r)
}

[[2]]$visible
[1] FALSE


[[3]]
[[3]]$value
function (phy, network, pars)
{
    if (sum(phy$edge.length == 0) > 0)
        cat("Function probably will not work when tree has zero-length branches\n\n")
    mm <- match(phy$tip.label, rownames(network))
    network <- network[mm, mm]
    nNodes <- max(phy$edge)
    nTips <- length(phy$tip.label)
    lik <- list()
    q01 <- pars[1]
    q10 <- pars[2]
    pSpec <- pars[3]
    if (pSpec > 1)
        pSpec <- 1
    qMatrix <- rbind(c(-q01, q01), c(q10, -q10))
    for (i in 1:nTips) {
        theTip <- phy$tip.label[i]
        theRow <- which(rownames(network) == theTip)
        interact <- network[theRow, ]
        rr <- matrix(nrow = nTips, ncol = 2)
        rownames(rr) <- phy$tip.label
        colnames(rr) <- c(0, 1)
        for (j in 1:nTips) if (interact[j] == 0)
            rr[j, ] <- c(0, -Inf)
        else rr[j, ] <- c(-Inf, 0)
        lik[[i]] <- rr
        names(lik)[i] <- theTip
    }
    currTree <- phy
    cumlnL <- 0
    while (1) {
        bt <- branching.times(currTree)
        recentNode <- names(bt)[which(bt == min(bt))][1]
        daughter <- tips(currTree, as.numeric(recentNode))
        timeInterval <- bt[recentNode]
        d1 <- which(names(lik) == daughter[1])
        d1L <- lik[[d1]]
        d2 <- which(names(lik) == daughter[2])
        d2L <- lik[[d2]]
        dCon <- d1L[daughter[2], ]
        dNode <- branchLike(dCon, timeInterval, qMatrix)
        dd <- dNode + log(c(1 - pSpec, pSpec))
        cumlnL <- cumlnL + -logspace_sum(dd)
        if (length(currTree$tip.label) == 2)
            break
        currTree <- drop.tip(currTree, daughter[1])
        oldLik <- lik
        lik <- list()
        nn <- length(currTree$tip.label)
        for (i in 1:nn) {
            theTip <- currTree$tip.label[i]
            rr <- matrix(nrow = nn, ncol = 2)
            rownames(rr) <- currTree$tip.label
            colnames(rr) <- c(0, 1)
            if (theTip %in% daughter) {
                xx <- which(names(oldLik) == daughter[1])
                ol1 <- oldLik[[xx]]
                xx <- which(names(oldLik) == daughter[2])
                ol2 <- oldLik[[xx]]
                for (j in 1:nrow(ol1)) {
                  if (rownames(ol1)[j] != daughter[1]) {
                    l1 <- branchLike(ol1[j, ], timeInterval,
                      qMatrix)
                    l2 <- branchLike(ol2[j, ], timeInterval,
                      qMatrix)
                    rr[rownames(ol1)[j], ] <- l1 + l2
                  }
                }
            }
            else {
                xx <- which(names(oldLik) == theTip)
                theOldLik <- oldLik[[xx]]
                lMerg <- theOldLik[daughter, ]
                l1 <- branchLike(lMerg[1, ], timeInterval, qMatrix)
                l2 <- branchLike(lMerg[2, ], timeInterval, qMatrix)
                rr[daughter[2], ] <- l1 + l2
                theRest <- rownames(rr)[which(!(rownames(rr) %in%
                  daughter))]
                for (tt in theRest) {
                  lold <- theOldLik[tt, ]
                  lnew <- branchLike(lold, timeInterval, qMatrix)
                  rr[tt, ] <- lnew
                }
            }
            rr[theTip, ] <- c(0, -Inf)
            lik[[i]] <- rr
            names(lik)[i] <- theTip
            nEdge <- which(currTree$edge[, 2] == i)
            currTree$edge.length[nEdge] <- currTree$edge.length[nEdge] -
                timeInterval
        }
        if (length(lik) == 80)
            break
    }
    return(cumlnL)
}

[[3]]$visible
[1] FALSE


[[4]]
[[4]]$value
function (tip.like, bl, q)
{
    nb.states <- length(tip.like)
    r <- rep(0, nb.states)
    p <- MatrixExp.eig(q * bl)
    for (i in 1:nb.states) r[i] <- logspace_sum(log(p[i, ]) +
        tip.like)
    return(r)
}

[[4]]$visible
[1] FALSE


[[5]]
[[5]]$value
function (logx, logy)
{
    if (logx == -Inf)
        return(logy)
    else max(logx, logy) + log1p(exp(-abs(logx - logy)))
}

[[5]]$visible
[1] FALSE


[[6]]
[[6]]$value
function (logx)
{
    r <- logx[1]
    if (length(logx) > 1)
        for (i in 2:length(logx)) r <- logspace_add(r, logx[i])
    r
}

[[6]]$visible
[1] FALSE


[[7]]
[[7]]$value
function (phy, qRate, sProb)
{
    bt <- sort(branching.times(phy), decr = T)
    nb <- dim(phy$edge)[1]
    interactionMatrix <- matrix(nrow = nb, ncol = nb)
    interactionMatrix[] <- 0
    currentEdges <- numeric()
    rootNumber <- as.numeric(names(bt)[1])
    qMatrix <- rbind(c(-1, 1), c(1, -1)) * qRate
    for (i in 1:length(bt)) {
        thisEdge <- as.numeric(names(bt)[i])
        ancestorRow <- which(phy$edge[, 2] == thisEdge)
        descendantRow <- which(phy$edge[, 1] == thisEdge)
        if (length(ancestorRow) == 0) {
            r <- runif(1)
            if (r < sProb) {
                interactionMatrix[descendantRow, descendantRow] <- 1
                diag(interactionMatrix) <- 0
            }
            currentEdges <- c(currentEdges, descendantRow)
        }
        else {
            timeSpan <- bt[i - 1] - bt[i]
            tProb <- MatrixExp.eig(qMatrix * timeSpan)
            cn <- length(currentEdges)
            for (j in 1:(cn - 1)) for (k in (j + 1):cn) {
                e1 <- currentEdges[j]
                e2 <- currentEdges[k]
                currState <- interactionMatrix[e1, e2]
                p0 <- tProb[currState + 1, 1]
                r <- runif(1)
                if (r < p0) {
                  interactionMatrix[e1, e2] <- 0
                  interactionMatrix[e2, e1] <- 0
                }
                else {
                  interactionMatrix[e1, e2] <- 1
                  interactionMatrix[e2, e1] <- 1
                }
            }
            toCut <- which(currentEdges == ancestorRow)
            currentEdges <- currentEdges[-toCut]
            currentEdges <- c(currentEdges, descendantRow)
            for (j in 1:length(descendantRow)) {
                interactionMatrix[, descendantRow[j]] <- interactionMatrix[,
                  ancestorRow]
                interactionMatrix[descendantRow[j], ] <- interactionMatrix[ancestorRow,
                  ]
                diag(interactionMatrix) <- 0
            }
            r <- runif(1)
            if (r < sProb) {
                interactionMatrix[descendantRow, descendantRow] <- 1
            }
        }
    }
    nTaxa <- length(phy$tip.label)
    tips <- which(phy$edge[, 2] <= nTaxa)
    oo <- phy$edge[tips, 2]
    res <- interactionMatrix[tips, tips]
    rownames(res) <- phy$tip.label[oo]
    colnames(res) <- phy$tip.label[oo]
    return(res)
}

[[7]]$visible
[1] FALSE


> fitPhyloNetwork(tt, xx, c(0.1, 0.1, 0.5))
Error in rownames(network) (from fitPhyloNetwork.R#12) : object 'xx' not found
> xx<-simPhyloNetwork(tt, qRate=0.1, sProb=0.5)
> fitPhyloNetwork(tt, xx, c(0.1, 0.1, 0.5))
[1] 394.526
> foo<-function(x) {
+   q01<-exp(x[1])
+   q10<-exp(x[2])
+   pSpec<-exp(x[3])
+   fitPhyloNetwork(tt, xx, c(0.1, 0.1, 0.5))
+ }
> foo(log(c(0.1, 0.1, 0.5)))
[1] 394.526
> out<-optim(log(c(0.5, 0.5, 0.5)), foo, control=list(trace=6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 394.526024
  Scaled convergence tolerance is 5.8789e-06
Stepsize computed as 0.069315
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.5 0.5 0.5
> lnlSurf<-matrix(nrow=9, ncol=9)
> for(fr in 1:9)
+   for(br in 1:9)
+     lnlSurf[fr, br]<-fitPhyloNetwork(tt, xx, c(fr/45, br/45, 0.5))
  C-c C-c
> contour(lnlSurf, levels=320:340, col="red", x=1:9/45, y=1:9/45)
> contour(lnlSurf, , x=1:9/45, y=1:9/45, add=T)
> sProb <- c(0,2)  # pSpec = probability of speciation within the network
> qProb <- c(0,1)  # transition probability between interacting and not-interacting
> tProb <- c(10,100)  # total number of taxa in the tree
> spProb <- c(0,2)  # lambda for tree simulation
> pSpec <- runif(1, sProb[1], sProb[2])
> q01 <- runif(1, qProb[1], qProb[2])
> q10 <- runif(1, qProb[1], qProb[2])
> lambda <- runif(1, spProb[1], spProb[2])
> ntaxa <- round(runif(1, tProb[1], tProb[2]))
> tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
> tt <- drop.tip(tt, paste0("s", ntaxa))
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(par[1], par[2], par[3]))
+ }
> foo(log(c(q01, q10, pSpec)))
Error in if (logx == -Inf) return(logy) else max(logx, logy) + log1p(exp(-abs(logx -  (from logspace_add.R#3) :
  missing value where TRUE/FALSE needed
In addition: Warning messages:
1: In log(p[i, ]) : NaNs produced
2: In log(p[i, ]) : NaNs produced
> tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(par[1], par[2], par[3]))
+ }
> foo(log(c(q01, q10, pSpec)))
Error in if (logx == -Inf) return(logy) else max(logx, logy) + log1p(exp(-abs(logx -  (from logspace_add.R#3) :
  missing value where TRUE/FALSE needed
In addition: Warning messages:
1: In log(p[i, ]) : NaNs produced
2: In log(p[i, ]) : NaNs produced
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
+ }
> foo(log(c(q01, q10, pSpec)))
[1] 312.9634
> out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 312.963366
  Scaled convergence tolerance is 4.66352e-06
Stepsize computed as 0.069315
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.5 0.5 0.5
> lnlSurf<-matrix(nrow=9, ncol=9)
> for(fr in 1:9)
+   for(br in 1:9)
+     lnlSurf[fr, br]<-fitPhyloNetwork(tt, net, c(fr/45, br/45, 0.5))
> contour(lnlSurf, levels=320:340, col="red", x=1:9/45, y=1:9/45)
> contour(lnlSurf, , x=1:9/45, y=1:9/45, add=T)
> pSpec <- runif(1, sProb[1], sProb[2])
> q01 <- runif(1, qProb[1], qProb[2])
> q10 <- q01
> lambda <- runif(1, spProb[1], spProb[2])
> ntaxa <- round(runif(1, tProb[1], tProb[2]))
> tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
> tt

Phylogenetic tree with 61 tips and 60 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> tt <- drop.tip(tt, paste0("s", ntaxa))
> tt

Phylogenetic tree with 60 tips and 59 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> net <- simPhyloNetwork(tt, qRate = q01, sProb = pSpec)
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(median(qProb), median(qProb), median(sProb)))
+ }
> foo(log(c(q01, q10, pSpec)))
[1] 1038.74
> out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 1038.740002
  Scaled convergence tolerance is 1.54784e-05
Stepsize computed as 0.069315
Exiting from Nelder Mead minimizer
    4 function evaluations used
> exp(out$par)
[1] 0.5 0.5 0.5
> foo <- function(par) {
+     q01 <- exp(par[1])
+     q10 <- exp(par[2])
+     pSpec <- exp(par[3])
+     fitPhyloNetwork(tt, net, c(q01, q10, pSpec))
+ }
> foo(log(c(q01, q10, pSpec)))
[1] 1023.419
> out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 1046.306520
  Scaled convergence tolerance is 1.55912e-05
Stepsize computed as 0.069315
BUILD              4 1046.306520 1042.990911
EXTENSION          6 1045.549282 1040.706679
LO-REDUCTION       8 1045.227274 1040.706679
EXTENSION         10 1042.990911 1036.593517
EXTENSION         12 1041.404047 1032.660249
EXTENSION         14 1040.706679 1028.999746
LO-REDUCTION      16 1036.593517 1028.999746
EXTENSION         18 1032.660249 1023.970494
LO-REDUCTION      20 1030.751396 1023.970494
LO-REDUCTION      22 1028.999746 1023.970494
LO-REDUCTION      24 1026.077591 1023.970494
REFLECTION        26 1025.006446 1023.349875
REFLECTION        28 1024.314464 1023.154675
EXTENSION         30 1023.970494 1021.571611
LO-REDUCTION      32 1023.349875 1021.571611
LO-REDUCTION      34 1023.154675 1021.571611
LO-REDUCTION      36 1022.309349 1021.545289
REFLECTION        38 1021.880064 1021.226373
LO-REDUCTION      40 1021.571611 1021.226373
LO-REDUCTION      42 1021.545289 1021.226373
LO-REDUCTION      44 1021.297961 1021.197190
HI-REDUCTION      46 1021.282680 1021.197190
LO-REDUCTION      48 1021.256564 1021.197190
LO-REDUCTION      50 1021.241547 1021.197190
LO-REDUCTION      52 1021.226373 1021.189456
LO-REDUCTION      54 1021.205836 1021.187734
REFLECTION        56 1021.197190 1021.184284
REFLECTION        58 1021.189456 1021.179179
HI-REDUCTION      60 1021.187734 1021.179106
HI-REDUCTION      62 1021.184284 1021.178262
HI-REDUCTION      64 1021.179179 1021.176683
HI-REDUCTION      66 1021.179106 1021.176317
LO-REDUCTION      68 1021.178262 1021.175801
LO-REDUCTION      70 1021.176683 1021.175234
LO-REDUCTION      72 1021.176317 1021.175020
LO-REDUCTION      74 1021.175801 1021.175020
LO-REDUCTION      76 1021.175234 1021.175020
REFLECTION        78 1021.175177 1021.174567
HI-REDUCTION      80 1021.175031 1021.174567
LO-REDUCTION      82 1021.175020 1021.174567
LO-REDUCTION      84 1021.174709 1021.174567
LO-REDUCTION      86 1021.174702 1021.174567
REFLECTION        88 1021.174608 1021.174506
LO-REDUCTION      90 1021.174572 1021.174498
HI-REDUCTION      92 1021.174567 1021.174498
EXTENSION         94 1021.174509 1021.174441
LO-REDUCTION      96 1021.174506 1021.174441
EXTENSION         98 1021.174498 1021.174346
LO-REDUCTION     100 1021.174443 1021.174346
HI-REDUCTION     102 1021.174441 1021.174346
EXTENSION        104 1021.174396 1021.174232
LO-REDUCTION     106 1021.174392 1021.174232
REFLECTION       108 1021.174346 1021.174215
EXTENSION        110 1021.174259 1021.174002
LO-REDUCTION     112 1021.174232 1021.174002
EXTENSION        114 1021.174215 1021.173982
EXTENSION        116 1021.174066 1021.173693
LO-REDUCTION     118 1021.174002 1021.173693
LO-REDUCTION     120 1021.173982 1021.173693
EXTENSION        122 1021.173790 1021.173543
EXTENSION        124 1021.173721 1021.173458
LO-REDUCTION     126 1021.173693 1021.173458
LO-REDUCTION     128 1021.173543 1021.173458
HI-REDUCTION     130 1021.173518 1021.173458
HI-REDUCTION     132 1021.173518 1021.173458
EXTENSION        134 1021.173490 1021.173391
LO-REDUCTION     136 1021.173487 1021.173391
LO-REDUCTION     138 1021.173458 1021.173391
REFLECTION       140 1021.173427 1021.173378
EXTENSION        142 1021.173405 1021.173357
LO-REDUCTION     144 1021.173391 1021.173357
LO-REDUCTION     146 1021.173378 1021.173357
Exiting from Nelder Mead minimizer
    148 function evaluations used
> exp(out$par)
[1] 0.8504394 0.7718425 0.9794304
> c(q01, q10, pSpec)
[1] 0.9213137 0.9213137 0.9898653
> out
$par
[1] -0.16200206 -0.25897477 -0.02078411

$value
[1] 1021.173

$counts
function gradient
     148       NA

$convergence
[1] 0

$message
NULL

> tt

Phylogenetic tree with 60 tips and 59 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> foo(log(c(q01, q10, pSpec)))
[1] 1023.419
> res <- c(out$par, out$value, q01, q10, pSpec)
> res
[1]   -0.16200206   -0.25897477   -0.02078411 1021.17335707    0.92131366
[6]    0.92131366    0.98986534
> res <- c(exp(out$par), out$value, q01, q10, pSpec)
> res
[1]    0.8504394    0.7718425    0.9794304 1021.1733571    0.9213137
[6]    0.9213137    0.9898653
> res <- c(out$value, exp(out$par), q01, q10, pSpec)
> res
[1] 1021.1733571    0.8504394    0.7718425    0.9794304    0.9213137
[6]    0.9213137    0.9898653
> pSpec <- numeric(1000)
> pSpec[] <- NA
> pSpec
   [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
  [25] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
  [49] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
  [73] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
  [97] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [121] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [145] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [169] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [193] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [217] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [241] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [265] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [289] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [313] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [337] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [361] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [385] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [409] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [433] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [457] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [481] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [505] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [529] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [553] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [577] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [601] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [625] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [649] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [673] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [697] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [721] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [745] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [769] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [793] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [817] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [841] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [865] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [889] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [913] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [937] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [961] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
 [985] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
> trees <- as.list(rep(NA, 1000))
> trees[[1]]
[1] NA
> trees[1:10]
[[1]]
[1] NA

[[2]]
[1] NA

[[3]]
[1] NA

[[4]]
[1] NA

[[5]]
[1] NA

[[6]]
[1] NA

[[7]]
[1] NA

[[8]]
[1] NA

[[9]]
[1] NA

[[10]]
[1] NA

> pSpec <- numeric(1000)
> pSpec[] <- NA
> q01 <- numeric(1000)
> q01[] <- NA
> lambda <- numeric(1000)
> lambda[] <- NA
> ntaxa <- numeric(1000)
> ntaxa[] <- NA
> trees <- as.list(rep(NA, 1000))
> nets <- as.list(rep(NA, 1000))
> sym.trans <- function(x) {
+     pSpec[x] <- runif(1, sProb[1], sProb[2])
+     q01[x] <- runif(1, qProb[1], qProb[2])
+     q10[x] <- q01
+     lambda[x] <- runif(1, spProb[1], spProb[2])
+     ntaxa[x] <- round(runif(1, tProb[1], tProb[2]))
+
+
+     tt[[x]] <- sim.bdtree(b = lambda[x], d = 0, stop = "taxa", n = ntaxa[x])
+     tt[[x]] <- drop.tip(tt[[x]], paste0("s", ntaxa[x]))
+     net[[x]] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x])
+
+
+     foo <- function(par) {
+         q01 <- exp(par[1])
+         q10 <- exp(par[2])
+         pSpec <- exp(par[3])
+         fitPhyloNetwork(tt[[x]], net[[x]], c(q01[x], q10[x], pSpec[x]))
+     }
+
+     #foo(log(c(q01[x], q10[x], pSpec[x])))
+     out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
+     res <- c(x, out$value, exp(out$par), q01[x], q10[x], pSpec[x])
+     #exp(out$par)
+ }
> sym.trans(1)
Error in net[[x]] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x]) (from #11) :
  more elements supplied than there are to replace
In addition: Warning message:
In q10[x] <- q01 :
  number of items to replace is not a multiple of replacement length
> sym.trans <- function(x) {
+     pSpec[x] <- runif(1, sProb[1], sProb[2])
+     q01[x] <- runif(1, qProb[1], qProb[2])
+     q10[x] <- q01[x]
+     lambda[x] <- runif(1, spProb[1], spProb[2])
+     ntaxa[x] <- round(runif(1, tProb[1], tProb[2]))
+
+
+     tt[[x]] <- sim.bdtree(b = lambda[x], d = 0, stop = "taxa", n = ntaxa[x])
+     tt[[x]] <- drop.tip(tt[[x]], paste0("s", ntaxa[x]))
+     net[[x]] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x])
+
+
+     foo <- function(par) {
+         q01 <- exp(par[1])
+         q10 <- exp(par[2])
+         pSpec <- exp(par[3])
+         fitPhyloNetwork(tt[[x]], net[[x]], c(q01[x], q10[x], pSpec[x]))
+     }
+
+     #foo(log(c(q01[x], q10[x], pSpec[x])))
+     out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
+     res <- c(x, out$value, exp(out$par), q01[x], q10[x], pSpec[x])
+     #exp(out$par)
+ }
> sym.trans(1)
Error in net[[x]] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x]) (from #11) :
  more elements supplied than there are to replace
> x <- 1
> pSpec[x]
[1] NA
>     pSpec[x] <- runif(1, sProb[1], sProb[2])
> pSpec[x]
[1] 1.713426
>     q01[x] <- runif(1, qProb[1], qProb[2])
>     q10[x] <- q01[x]
> q01
   [1] 0.6749924        NA        NA        NA        NA        NA        NA
   [8]        NA        NA        NA        NA        NA        NA        NA
  [15]        NA        NA        NA        NA        NA        NA        NA
  [22]        NA        NA        NA        NA        NA        NA        NA
  [29]        NA        NA        NA        NA        NA        NA        NA
  [36]        NA        NA        NA        NA        NA        NA        NA
  [43]        NA        NA        NA        NA        NA        NA        NA
  [50]        NA        NA        NA        NA        NA        NA        NA
  [57]        NA        NA        NA        NA        NA        NA        NA
  [64]        NA        NA        NA        NA        NA        NA        NA
  [71]        NA        NA        NA        NA        NA        NA        NA
  [78]        NA        NA        NA        NA        NA        NA        NA
  [85]        NA        NA        NA        NA        NA        NA        NA
  [92]        NA        NA        NA        NA        NA        NA        NA
  [99]        NA        NA        NA        NA        NA        NA        NA
 [106]        NA        NA        NA        NA        NA        NA        NA
 [113]        NA        NA        NA        NA        NA        NA        NA
 [120]        NA        NA        NA        NA        NA        NA        NA
 [127]        NA        NA        NA        NA        NA        NA        NA
 [134]        NA        NA        NA        NA        NA        NA        NA
 [141]        NA        NA        NA        NA        NA        NA        NA
 [148]        NA        NA        NA        NA        NA        NA        NA
 [155]        NA        NA        NA        NA        NA        NA        NA
 [162]        NA        NA        NA        NA        NA        NA        NA
 [169]        NA        NA        NA        NA        NA        NA        NA
 [176]        NA        NA        NA        NA        NA        NA        NA
 [183]        NA        NA        NA        NA        NA        NA        NA
 [190]        NA        NA        NA        NA        NA        NA        NA
 [197]        NA        NA        NA        NA        NA        NA        NA
 [204]        NA        NA        NA        NA        NA        NA        NA
 [211]        NA        NA        NA        NA        NA        NA        NA
 [218]        NA        NA        NA        NA        NA        NA        NA
 [225]        NA        NA        NA        NA        NA        NA        NA
 [232]        NA        NA        NA        NA        NA        NA        NA
 [239]        NA        NA        NA        NA        NA        NA        NA
 [246]        NA        NA        NA        NA        NA        NA        NA
 [253]        NA        NA        NA        NA        NA        NA        NA
 [260]        NA        NA        NA        NA        NA        NA        NA
 [267]        NA        NA        NA        NA        NA        NA        NA
 [274]        NA        NA        NA        NA        NA        NA        NA
 [281]        NA        NA        NA        NA        NA        NA        NA
 [288]        NA        NA        NA        NA        NA        NA        NA
 [295]        NA        NA        NA        NA        NA        NA        NA
 [302]        NA        NA        NA        NA        NA        NA        NA
 [309]        NA        NA        NA        NA        NA        NA        NA
 [316]        NA        NA        NA        NA        NA        NA        NA
 [323]        NA        NA        NA        NA        NA        NA        NA
 [330]        NA        NA        NA        NA        NA        NA        NA
 [337]        NA        NA        NA        NA        NA        NA        NA
 [344]        NA        NA        NA        NA        NA        NA        NA
 [351]        NA        NA        NA        NA        NA        NA        NA
 [358]        NA        NA        NA        NA        NA        NA        NA
 [365]        NA        NA        NA        NA        NA        NA        NA
 [372]        NA        NA        NA        NA        NA        NA        NA
 [379]        NA        NA        NA        NA        NA        NA        NA
 [386]        NA        NA        NA        NA        NA        NA        NA
 [393]        NA        NA        NA        NA        NA        NA        NA
 [400]        NA        NA        NA        NA        NA        NA        NA
 [407]        NA        NA        NA        NA        NA        NA        NA
 [414]        NA        NA        NA        NA        NA        NA        NA
 [421]        NA        NA        NA        NA        NA        NA        NA
 [428]        NA        NA        NA        NA        NA        NA        NA
 [435]        NA        NA        NA        NA        NA        NA        NA
 [442]        NA        NA        NA        NA        NA        NA        NA
 [449]        NA        NA        NA        NA        NA        NA        NA
 [456]        NA        NA        NA        NA        NA        NA        NA
 [463]        NA        NA        NA        NA        NA        NA        NA
 [470]        NA        NA        NA        NA        NA        NA        NA
 [477]        NA        NA        NA        NA        NA        NA        NA
 [484]        NA        NA        NA        NA        NA        NA        NA
 [491]        NA        NA        NA        NA        NA        NA        NA
 [498]        NA        NA        NA        NA        NA        NA        NA
 [505]        NA        NA        NA        NA        NA        NA        NA
 [512]        NA        NA        NA        NA        NA        NA        NA
 [519]        NA        NA        NA        NA        NA        NA        NA
 [526]        NA        NA        NA        NA        NA        NA        NA
 [533]        NA        NA        NA        NA        NA        NA        NA
 [540]        NA        NA        NA        NA        NA        NA        NA
 [547]        NA        NA        NA        NA        NA        NA        NA
 [554]        NA        NA        NA        NA        NA        NA        NA
 [561]        NA        NA        NA        NA        NA        NA        NA
 [568]        NA        NA        NA        NA        NA        NA        NA
 [575]        NA        NA        NA        NA        NA        NA        NA
 [582]        NA        NA        NA        NA        NA        NA        NA
 [589]        NA        NA        NA        NA        NA        NA        NA
 [596]        NA        NA        NA        NA        NA        NA        NA
 [603]        NA        NA        NA        NA        NA        NA        NA
 [610]        NA        NA        NA        NA        NA        NA        NA
 [617]        NA        NA        NA        NA        NA        NA        NA
 [624]        NA        NA        NA        NA        NA        NA        NA
 [631]        NA        NA        NA        NA        NA        NA        NA
 [638]        NA        NA        NA        NA        NA        NA        NA
 [645]        NA        NA        NA        NA        NA        NA        NA
 [652]        NA        NA        NA        NA        NA        NA        NA
 [659]        NA        NA        NA        NA        NA        NA        NA
 [666]        NA        NA        NA        NA        NA        NA        NA
 [673]        NA        NA        NA        NA        NA        NA        NA
 [680]        NA        NA        NA        NA        NA        NA        NA
 [687]        NA        NA        NA        NA        NA        NA        NA
 [694]        NA        NA        NA        NA        NA        NA        NA
 [701]        NA        NA        NA        NA        NA        NA        NA
 [708]        NA        NA        NA        NA        NA        NA        NA
 [715]        NA        NA        NA        NA        NA        NA        NA
 [722]        NA        NA        NA        NA        NA        NA        NA
 [729]        NA        NA        NA        NA        NA        NA        NA
 [736]        NA        NA        NA        NA        NA        NA        NA
 [743]        NA        NA        NA        NA        NA        NA        NA
 [750]        NA        NA        NA        NA        NA        NA        NA
 [757]        NA        NA        NA        NA        NA        NA        NA
 [764]        NA        NA        NA        NA        NA        NA        NA
 [771]        NA        NA        NA        NA        NA        NA        NA
 [778]        NA        NA        NA        NA        NA        NA        NA
 [785]        NA        NA        NA        NA        NA        NA        NA
 [792]        NA        NA        NA        NA        NA        NA        NA
 [799]        NA        NA        NA        NA        NA        NA        NA
 [806]        NA        NA        NA        NA        NA        NA        NA
 [813]        NA        NA        NA        NA        NA        NA        NA
 [820]        NA        NA        NA        NA        NA        NA        NA
 [827]        NA        NA        NA        NA        NA        NA        NA
 [834]        NA        NA        NA        NA        NA        NA        NA
 [841]        NA        NA        NA        NA        NA        NA        NA
 [848]        NA        NA        NA        NA        NA        NA        NA
 [855]        NA        NA        NA        NA        NA        NA        NA
 [862]        NA        NA        NA        NA        NA        NA        NA
 [869]        NA        NA        NA        NA        NA        NA        NA
 [876]        NA        NA        NA        NA        NA        NA        NA
 [883]        NA        NA        NA        NA        NA        NA        NA
 [890]        NA        NA        NA        NA        NA        NA        NA
 [897]        NA        NA        NA        NA        NA        NA        NA
 [904]        NA        NA        NA        NA        NA        NA        NA
 [911]        NA        NA        NA        NA        NA        NA        NA
 [918]        NA        NA        NA        NA        NA        NA        NA
 [925]        NA        NA        NA        NA        NA        NA        NA
 [932]        NA        NA        NA        NA        NA        NA        NA
 [939]        NA        NA        NA        NA        NA        NA        NA
 [946]        NA        NA        NA        NA        NA        NA        NA
 [953]        NA        NA        NA        NA        NA        NA        NA
 [960]        NA        NA        NA        NA        NA        NA        NA
 [967]        NA        NA        NA        NA        NA        NA        NA
 [974]        NA        NA        NA        NA        NA        NA        NA
 [981]        NA        NA        NA        NA        NA        NA        NA
 [988]        NA        NA        NA        NA        NA        NA        NA
 [995]        NA        NA        NA        NA        NA        NA
> q01[x]
[1] 0.6749924
> q10[x]
[1] 0.6749924
>     lambda[x] <- runif(1, spProb[1], spProb[2])
> lambda[x]
[1] 0.006425726
> ntaxa[x]
[1] NA
>     ntaxa[x] <- round(runif(1, tProb[1], tProb[2]))
> ntaxa[x]
[1] 84
>     tt[[x]] <- sim.bdtree(b = lambda[x], d = 0, stop = "taxa", n = ntaxa[x])
> tt[[x]]

Phylogenetic tree with 84 tips and 83 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> tt[[x]]

Phylogenetic tree with 84 tips and 83 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
>     tt[[x]] <- drop.tip(tt[[x]], paste0("s", ntaxa[x]))
> tt[[x]]

Phylogenetic tree with 83 tips and 82 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
>     net[[x]] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x])
Error in net[[x]] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x]) :
  more elements supplied than there are to replace
> tt[[x]]

Phylogenetic tree with 83 tips and 82 internal nodes.

Tip labels:
	s1, s2, s3, s4, s5, s6, ...

Rooted; includes branch lengths.
> q01[x]
[1] 0.6749924
> pSpec[x]
[1] 1.713426
> simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x])
    s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 s19 s20 s21
s1   0  0  0  1  1  1  0  1  1   0   0   0   1   1   0   1   1   1   0   0   1
s2   0  0  1  0  1  0  1  1  1   0   1   0   1   0   1   0   0   1   0   0   0
s3   0  1  0  0  0  1  1  0  0   0   0   0   1   1   0   1   1   0   0   1   1
s4   1  0  0  0  0  1  0  1  1   1   1   0   1   0   1   1   1   1   0   0   0
s5   1  1  0  0  0  1  0  0  1   1   1   1   1   1   0   0   0   0   0   1   1
s6   1  0  1  1  1  0  0  1  1   0   1   1   0   1   1   0   1   0   0   1   1
s7   0  1  1  0  0  0  0  0  0   1   0   0   1   1   1   1   0   0   0   1   1
s8   1  1  0  1  0  1  0  0  0   0   1   0   0   0   0   1   0   1   1   1   0
s9   1  1  0  1  1  1  0  0  0   1   1   0   1   1   1   0   0   0   1   1   0
s10  0  0  0  1  1  0  1  0  1   0   0   1   1   0   0   1   1   1   0   0   0
s11  0  1  0  1  1  1  0  1  1   0   0   1   0   1   0   1   1   0   0   0   1
s12  0  0  0  0  1  1  0  0  0   1   1   0   1   1   0   1   1   0   0   0   0
s13  1  1  1  1  1  0  1  0  1   1   0   1   0   1   0   1   0   1   0   1   1
s14  1  0  1  0  1  1  1  0  1   0   1   1   1   0   1   1   1   1   0   0   0
s15  0  1  0  1  0  1  1  0  1   0   0   0   0   1   0   1   1   0   1   1   1
s16  1  0  1  1  0  0  1  1  0   1   1   1   1   1   1   0   1   1   1   1   0
s17  1  0  1  1  0  1  0  0  0   1   1   1   0   1   1   1   0   0   0   1   0
s18  1  1  0  1  0  0  0  1  0   1   0   0   1   1   0   1   0   0   1   1   1
s19  0  0  0  0  0  0  0  1  1   0   0   0   0   0   1   1   0   1   0   0   0
s20  0  0  1  0  1  1  1  1  1   0   0   0   1   0   1   1   1   1   0   0   1
s21  1  0  1  0  1  1  1  0  0   0   1   0   1   0   1   0   0   1   0   1   0
s22  1  1  0  1  0  1  1  0  1   0   0   0   1   0   1   1   1   0   1   1   0
s23  0  1  1  1  1  0  0  1  0   1   0   0   0   0   1   1   0   1   1   0   0
s24  1  0  1  0  1  0  1  1  1   1   0   1   1   0   0   0   0   1   1   0   1
s25  1  0  0  1  1  0  1  1  0   1   0   1   0   1   1   1   1   1   0   0   0
s26  0  0  1  0  1  1  0  0  0   0   1   1   1   1   0   0   0   0   0   0   0
s27  0  0  1  0  1  0  0  1  0   0   1   1   0   0   1   0   0   1   0   1   1
s28  0  1  0  0  0  1  1  0  1   0   0   1   0   0   1   1   0   0   0   0   1
s29  1  0  1  0  1  1  0  1  0   1   0   1   1   0   0   0   0   1   0   0   0
s30  0  1  0  0  0  1  0  0  0   1   0   0   1   0   0   0   1   0   1   1   1
s31  1  1  0  0  1  1  1  1  1   0   1   1   1   0   1   1   1   0   1   1   1
s32  0  0  0  1  0  0  0  1  1   0   1   0   1   1   1   1   0   1   0   0   1
s33  1  1  1  0  0  1  1  1  1   1   1   0   0   1   1   1   0   0   1   0   0
s34  0  1  1  1  0  1  0  1  0   1   0   0   1   1   1   0   0   1   0   0   0
s35  1  1  1  1  1  1  0  1  1   0   1   1   1   1   0   0   0   1   1   0   1
s36  1  1  0  0  0  0  1  0  1   0   0   1   1   1   1   0   1   0   0   1   0
s37  1  1  0  1  0  0  1  1  0   1   1   0   1   0   0   0   0   1   0   1   0
s38  1  0  1  0  0  0  0  1  1   1   1   0   1   0   1   0   0   1   1   0   0
s39  0  0  1  0  0  1  0  0  0   1   0   1   1   0   1   0   1   0   0   0   0
s40  0  1  1  1  0  0  1  1  0   0   0   0   0   0   0   0   1   1   1   1   0
s41  0  1  0  0  1  0  1  1  1   0   0   1   0   1   0   1   1   1   0   0   0
s42  0  0  1  0  1  0  1  0  0   1   1   1   1   0   0   0   1   1   1   1   1
s43  0  1  1  0  1  1  1  0  1   0   1   1   1   1   0   0   0   1   0   0   0
s44  1  0  1  1  0  1  0  1  0   1   1   0   0   0   1   0   1   0   0   0   1
s45  1  0  0  0  1  0  0  1  0   0   0   0   1   0   1   1   0   1   0   1   1
s46  1  1  0  0  1  0  1  1  0   0   1   1   1   0   1   1   1   1   1   0   1
s47  1  1  1  0  0  0  1  0  0   1   1   0   0   0   1   0   1   0   0   1   0
s48  0  0  1  0  1  0  0  1  0   0   1   0   1   1   1   1   0   0   1   0   0
s49  0  1  0  1  0  1  0  0  0   0   1   0   0   1   1   1   1   1   1   1   1
s50  1  0  0  1  1  0  1  1  1   1   1   1   0   0   1   0   1   1   0   1   0
s51  1  0  1  0  0  1  1  1  1   0   0   0   1   1   0   0   1   0   0   0   0
s52  1  1  1  1  1  1  0  1  1   0   0   1   0   0   1   1   1   0   1   1   0
s53  1  0  0  1  0  0  0  1  1   1   0   1   0   1   0   0   1   0   1   1   1
s54  0  1  1  0  0  1  0  1  1   0   0   0   1   1   0   0   1   0   1   1   1
s55  1  1  0  0  0  0  1  0  0   0   0   1   1   0   0   0   1   0   1   1   0
s56  0  1  0  1  0  0  1  0  1   1   1   1   1   1   1   1   1   0   0   0   0
s57  0  1  1  0  0  1  0  0  1   1   1   1   0   0   1   0   1   0   1   0   1
s58  0  1  1  0  0  1  0  0  1   1   1   1   0   0   1   0   1   0   1   0   1
s59  0  1  0  1  1  0  1  0  1   1   1   1   0   1   0   1   1   0   1   0   0
s60  0  0  1  0  1  1  1  0  1   1   1   0   0   1   0   1   1   0   1   1   0
s61  0  0  0  0  0  0  0  1  1   1   1   0   1   1   1   0   1   0   1   1   0
s62  1  0  0  0  1  0  0  1  1   0   1   1   0   0   0   1   1   1   0   1   1
s63  0  1  1  1  1  0  1  1  0   0   0   0   1   1   1   0   0   1   1   0   0
s64  0  0  1  0  1  0  0  1  1   0   0   0   0   1   0   1   0   1   0   0   1
s65  0  1  0  1  0  0  0  0  0   1   1   0   0   0   1   0   1   1   0   0   0
s66  0  0  0  1  1  0  0  1  1   1   0   0   1   1   0   1   1   1   1   1   0
s67  0  1  1  0  0  1  1  0  0   1   1   0   1   1   1   1   1   0   0   1   0
s68  0  1  0  0  1  1  1  1  1   1   1   1   1   1   0   0   0   1   0   0   1
s69  0  1  0  1  1  1  0  0  1   0   1   1   1   1   0   0   0   0   0   0   1
s70  1  0  0  0  1  0  1  1  0   1   0   0   0   1   1   0   1   0   1   0   1
s71  0  0  1  1  0  1  0  1  1   0   0   1   0   0   1   0   0   1   0   1   1
s72  1  1  0  1  0  0  0  0  0   0   1   1   1   1   0   1   1   1   0   1   0
s73  1  0  1  1  0  1  0  0  1   0   1   1   1   1   1   1   0   0   0   1   0
s74  1  1  1  1  1  0  0  0  1   0   1   1   0   0   0   1   0   0   1   1   1
s75  0  0  0  1  0  0  1  0  1   1   0   1   0   0   0   1   0   1   0   1   0
s76  0  1  1  1  1  0  0  0  0   0   0   0   1   0   0   0   1   1   0   0   0
s77  1  1  1  1  0  1  1  0  1   0   0   1   0   0   0   0   0   1   0   0   1
s78  0  1  1  1  1  1  0  1  1   0   0   0   1   1   1   1   1   1   0   1   0
s79  0  0  0  0  0  1  1  0  1   0   0   0   0   0   1   1   1   0   0   1   1
s80  0  1  0  1  0  1  0  0  0   1   1   1   0   1   0   1   1   1   0   1   0
s81  0  0  1  1  1  0  1  0  1   0   0   1   0   0   0   1   0   1   1   0   0
s82  1  1  0  0  1  1  0  0  1   1   1   1   0   0   0   1   0   0   0   0   1
s83  0  0  1  1  0  1  0  0  0   0   1   1   1   0   0   0   0   1   0   0   1
    s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 s36 s37 s38 s39 s40
s1    1   0   1   1   0   0   0   1   0   1   0   1   0   1   1   1   1   0   0
s2    1   1   0   0   0   0   1   0   1   1   0   1   1   1   1   1   0   0   1
s3    0   1   1   0   1   1   0   1   0   0   0   1   1   1   0   0   1   1   1
s4    1   1   0   1   0   0   0   0   0   0   1   0   1   1   0   1   0   0   1
s5    0   1   1   1   1   1   0   1   0   1   0   0   0   1   0   0   0   0   0
s6    1   0   0   0   1   0   1   1   1   1   0   1   1   1   0   0   0   1   0
s7    1   0   1   1   0   0   1   0   0   1   0   1   0   0   1   1   0   0   1
s8    0   1   1   1   0   1   0   1   0   1   1   1   1   1   0   1   1   0   1
s9    1   0   1   0   0   0   1   0   0   1   1   1   0   1   1   0   1   0   0
s10   0   1   1   1   0   0   0   1   1   0   0   1   1   0   0   1   1   1   0
s11   0   0   0   0   1   1   0   0   0   1   1   1   0   1   0   1   1   0   0
s12   0   0   1   1   1   1   1   1   0   1   0   0   0   1   1   0   0   1   0
s13   1   0   1   0   1   0   0   1   1   1   1   0   1   1   1   1   1   1   0
s14   0   0   0   1   1   0   0   0   0   0   1   1   1   1   1   0   0   0   0
s15   1   1   0   1   0   1   1   0   0   1   1   1   1   0   1   0   1   1   0
s16   1   1   0   1   0   0   1   0   0   1   1   1   0   0   0   0   0   0   0
s17   1   0   0   1   0   0   0   0   1   1   0   0   0   0   1   0   0   1   1
s18   0   1   1   1   0   1   0   1   0   0   1   0   1   1   0   1   1   0   1
s19   1   1   1   0   0   0   0   0   1   1   0   1   0   1   0   0   1   0   1
s20   1   0   0   0   0   1   0   0   1   1   0   0   0   0   1   1   0   0   1
s21   0   0   1   0   0   1   1   0   1   1   1   0   0   1   0   0   0   0   0
s22   0   0   0   0   1   0   0   0   0   1   1   0   1   1   1   0   1   1   0
s23   0   0   1   0   0   1   1   0   1   1   1   1   0   1   1   0   1   1   0
s24   0   1   0   0   1   1   0   1   0   1   0   0   0   1   0   1   0   1   1
s25   0   0   0   0   0   1   0   0   1   0   0   0   0   1   1   1   1   1   1
s26   1   0   1   0   0   0   0   1   1   0   0   0   0   1   1   1   0   1   0
s27   0   1   1   1   0   0   1   1   0   0   0   1   0   0   0   1   0   1   0
s28   0   1   0   0   0   1   0   1   0   0   1   1   0   1   1   1   1   1   1
s29   0   0   1   0   1   1   1   0   0   0   1   1   1   1   0   1   0   1   1
s30   0   1   0   1   1   0   0   0   0   1   0   0   1   1   0   1   1   1   1
s31   1   1   1   0   0   0   0   0   1   0   1   0   0   0   0   1   1   1   1
s32   1   1   0   0   0   0   1   1   0   1   0   0   1   1   1   0   0   0   0
s33   0   1   0   0   0   1   1   1   0   0   0   0   0   0   1   0   0   0   1
s34   1   0   0   0   0   0   0   1   1   0   1   0   0   1   1   1   1   0   0
s35   1   1   1   1   1   0   1   1   1   0   1   0   1   0   1   1   0   1   1
s36   1   1   0   1   1   0   1   0   0   0   1   1   1   1   0   1   1   0   1
s37   0   0   1   1   1   1   1   1   1   1   0   0   1   1   1   0   1   1   0
s38   1   1   0   1   0   0   1   0   1   1   0   0   1   0   1   1   0   1   0
s39   1   1   1   1   1   1   1   1   1   1   0   0   0   1   0   1   1   0   0
s40   0   0   1   1   0   0   1   1   1   1   0   1   0   1   1   0   0   0   0
s41   1   1   0   1   0   1   1   1   0   0   0   0   1   0   1   0   1   1   1
s42   0   0   1   1   0   0   1   1   0   0   1   0   1   1   1   0   0   0   0
s43   1   1   0   0   0   0   1   0   1   1   0   0   1   0   0   1   1   0   1
s44   0   0   1   0   0   0   0   0   0   1   1   0   0   0   0   0   0   1   0
s45   0   1   0   1   0   1   1   0   1   0   0   0   1   0   0   1   0   1   1
s46   0   1   0   1   1   0   1   1   0   0   1   0   1   0   0   1   1   0   0
s47   0   1   1   1   0   0   1   1   1   1   0   1   1   1   0   0   0   1   0
s48   0   0   0   1   1   1   1   1   0   0   1   0   1   0   0   0   0   0   0
s49   1   0   1   0   0   1   0   1   1   0   1   0   1   0   0   1   1   1   1
s50   0   0   0   1   0   0   0   0   0   1   0   0   1   0   1   0   1   0   0
s51   0   0   0   0   0   0   0   0   0   1   1   0   1   1   0   0   1   0   1
s52   0   1   1   0   1   1   0   1   0   1   0   1   1   1   1   1   1   1   1
s53   0   0   0   1   1   1   1   0   1   1   1   0   0   1   0   1   1   1   1
s54   0   0   1   1   0   0   1   1   0   0   1   1   1   0   1   1   1   1   1
s55   0   0   0   0   0   1   1   0   1   1   0   1   1   1   0   0   1   0   1
s56   0   0   0   0   0   1   1   0   0   0   1   0   1   1   1   0   0   1   0
s57   0   1   0   0   1   0   1   0   0   0   1   0   1   0   1   1   1   1   1
s58   0   1   0   0   1   0   1   0   0   0   1   0   1   0   1   1   1   1   1
s59   0   1   1   0   0   1   1   1   0   0   1   0   1   1   1   1   1   0   0
s60   0   1   1   0   0   0   0   0   0   0   0   0   0   0   1   0   1   0   1
s61   1   0   0   0   1   0   1   1   0   1   1   1   1   0   0   0   1   0   0
s62   1   0   1   1   0   1   0   1   0   1   0   1   0   0   0   1   1   0   0
s63   0   1   1   1   1   0   0   0   1   0   1   1   0   1   0   1   0   0   0
s64   1   1   0   0   0   0   0   0   0   1   0   0   0   1   1   1   1   1   1
s65   1   0   0   1   0   0   0   0   0   0   0   1   0   0   0   0   1   1   0
s66   0   1   0   0   0   0   1   1   1   0   1   1   1   1   1   0   0   0   1
s67   0   0   1   0   1   1   1   1   1   1   0   1   1   1   1   1   0   1   1
s68   0   1   1   0   0   0   1   1   0   0   1   0   1   1   1   0   1   0   0
s69   1   0   1   0   1   1   0   1   0   0   0   0   0   0   0   0   1   0   0
s70   0   1   0   1   1   1   1   1   1   0   1   1   1   1   0   0   0   1   0
s71   0   0   1   1   0   1   0   1   0   1   1   1   1   1   0   0   1   1   1
s72   1   1   1   0   1   0   1   0   1   0   1   1   1   1   0   0   1   1   1
s73   0   0   1   0   0   0   0   0   0   0   1   0   1   1   1   1   0   0   1
s74   0   1   1   1   1   1   1   1   0   0   0   0   0   0   0   1   0   1   1
s75   1   1   1   1   0   1   0   1   1   1   1   0   1   0   1   0   0   0   0
s76   1   1   1   1   1   0   1   0   0   0   0   1   1   0   0   0   0   0   0
s77   1   1   0   0   0   1   0   1   0   0   1   1   1   1   1   0   1   0   1
s78   0   0   0   1   0   0   1   0   0   1   1   1   0   1   1   0   0   0   0
s79   1   1   1   0   1   0   1   1   0   0   0   0   1   0   0   0   1   0   1
s80   0   1   0   0   0   1   1   0   1   1   1   0   1   0   1   0   0   0   1
s81   1   1   1   0   1   0   1   0   0   1   0   0   1   0   1   1   1   0   0
s82   0   0   0   1   0   1   1   1   1   0   1   1   1   1   0   0   0   1   0
s83   0   1   0   0   1   0   0   1   1   1   1   0   0   1   1   0   1   0   1
    s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 s54 s55 s56 s57 s58 s59
s1    0   0   0   1   1   1   1   0   0   1   1   1   1   0   1   0   0   0   0
s2    1   0   1   0   0   1   1   0   1   0   0   1   0   1   1   1   1   1   1
s3    0   1   1   1   0   0   1   1   0   0   1   1   0   1   0   0   1   1   0
s4    0   0   0   1   0   0   0   0   1   1   0   1   1   0   0   1   0   0   1
s5    1   1   1   0   1   1   0   1   0   1   0   1   0   0   0   0   0   0   1
s6    0   0   1   1   0   0   0   0   1   0   1   1   0   1   0   0   1   1   0
s7    1   1   1   0   0   1   1   0   0   1   1   0   0   0   1   1   0   0   1
s8    1   0   0   1   1   1   0   1   0   1   1   1   1   1   0   0   0   0   0
s9    1   0   1   0   0   0   0   0   0   1   1   1   1   1   0   1   1   1   1
s10   0   1   0   1   0   0   1   0   0   1   0   0   1   0   0   1   1   1   1
s11   0   1   1   1   0   1   1   1   1   1   0   0   0   0   0   1   1   1   1
s12   1   1   1   0   0   1   0   0   0   1   0   1   1   0   1   1   1   1   1
s13   0   1   1   0   1   1   0   1   0   0   1   0   0   1   1   1   0   0   0
s14   1   0   1   0   0   0   0   1   1   0   1   0   1   1   0   1   0   0   1
s15   0   0   0   1   1   1   1   1   1   1   0   1   0   0   0   1   1   1   0
s16   1   0   0   0   1   1   0   1   1   0   0   1   0   0   0   1   0   0   1
s17   1   1   0   1   0   1   1   0   1   1   1   1   1   1   1   1   1   1   1
s18   1   1   1   0   1   1   0   0   1   1   0   0   0   0   0   0   0   0   0
s19   0   1   0   0   0   1   0   1   1   0   0   1   1   1   1   0   1   1   1
s20   0   1   0   0   1   0   1   0   1   1   0   1   1   1   1   0   0   0   0
s21   0   1   0   1   1   1   0   0   1   0   0   0   1   1   0   0   1   1   0
s22   1   0   1   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0
s23   1   0   1   0   1   1   1   0   0   0   0   1   0   0   0   0   1   1   1
s24   0   1   0   1   0   0   1   0   1   0   0   1   0   1   0   0   0   0   1
s25   1   1   0   0   1   1   1   1   0   1   0   0   1   1   0   0   0   0   0
s26   0   0   0   0   0   1   0   1   0   0   0   1   1   0   0   0   1   1   0
s27   1   0   0   0   1   0   0   1   1   0   0   1   1   0   1   1   0   0   1
s28   1   1   1   0   1   1   1   1   0   0   0   0   1   1   1   1   1   1   1
s29   1   1   0   0   0   1   1   1   1   0   0   1   0   1   0   0   0   0   1
s30   0   0   1   0   1   0   1   0   1   0   0   0   1   0   1   0   0   0   0
s31   0   0   1   1   0   0   1   0   0   1   1   1   1   0   1   0   0   0   0
s32   0   1   0   1   0   1   0   1   1   0   1   0   1   1   0   1   1   1   1
s33   0   0   0   0   0   0   1   0   0   0   0   1   0   1   1   0   0   0   0
s34   1   1   1   0   1   1   1   1   1   1   1   1   0   1   1   1   1   1   1
s35   0   1   0   0   0   0   1   0   0   0   1   1   1   0   1   1   0   0   1
s36   1   1   0   0   0   0   0   0   0   1   0   1   0   1   0   1   1   1   1
s37   0   0   1   0   1   1   0   0   1   0   0   1   1   1   0   0   1   1   1
s38   1   0   1   0   0   1   0   0   1   1   1   1   1   1   1   0   1   1   1
s39   1   0   0   1   1   0   1   0   1   0   0   1   1   1   0   1   1   1   0
s40   1   0   1   0   1   0   0   0   1   0   1   1   1   1   1   0   1   1   0
s41   0   1   1   1   0   1   1   1   0   1   0   1   1   0   0   1   0   0   0
s42   1   0   1   1   1   1   0   0   1   1   1   1   0   0   1   0   0   0   1
s43   1   1   0   1   0   1   0   0   1   0   1   0   0   1   0   1   0   0   1
s44   1   1   1   0   1   1   0   0   0   0   0   0   1   1   0   1   0   0   0
s45   0   1   0   1   0   1   0   0   1   0   1   0   0   1   1   1   1   1   0
s46   1   1   1   1   1   0   1   1   0   0   0   0   1   1   0   1   0   0   1
s47   1   0   0   0   0   1   0   1   1   0   1   0   1   1   0   1   1   1   0
s48   1   0   0   0   0   1   1   0   1   0   0   1   1   0   0   1   1   1   1
s49   0   1   1   0   1   0   1   1   0   1   0   0   0   0   0   0   0   0   1
s50   1   1   0   0   0   0   0   0   1   0   1   1   0   1   0   0   0   0   1
s51   0   1   1   0   1   0   1   0   0   1   0   1   1   0   0   1   0   0   1
s52   1   1   0   0   0   0   0   1   0   1   1   0   0   1   0   0   0   0   1
s53   1   0   0   1   0   1   1   1   0   0   1   0   0   1   0   1   0   0   0
s54   0   0   1   1   1   1   1   0   0   1   0   1   1   0   0   1   1   1   0
s55   0   1   0   0   1   0   0   0   0   0   0   0   0   0   0   1   0   0   1
s56   1   0   1   1   1   1   1   1   0   0   1   0   1   1   1   0   1   1   1
s57   0   0   0   0   1   0   1   1   0   0   0   0   0   1   0   1   1   1   0
s58   0   0   0   0   1   0   1   1   0   0   0   0   0   1   0   1   1   1   0
s59   0   1   1   0   0   1   0   1   1   1   1   1   0   0   1   1   0   0   0
s60   1   1   1   1   0   1   0   1   1   0   1   1   0   0   1   0   1   1   1
s61   0   1   0   1   1   1   0   1   1   1   0   0   0   0   1   1   1   1   1
s62   0   0   0   0   1   1   0   1   0   0   0   1   0   1   1   0   1   1   0
s63   1   1   1   1   1   1   1   1   0   1   0   0   0   0   1   1   1   1   1
s64   1   1   0   1   1   1   0   1   0   1   0   0   0   1   0   0   1   1   1
s65   0   0   1   1   0   1   1   0   0   0   0   0   0   1   1   1   0   0   1
s66   1   1   1   0   0   0   0   0   1   0   0   0   0   1   1   1   0   0   0
s67   0   1   0   1   1   1   0   1   0   1   1   1   1   1   0   0   1   1   0
s68   0   1   1   0   0   1   1   1   1   1   1   1   0   1   0   0   1   1   1
s69   0   1   0   1   0   1   1   1   1   1   0   1   0   1   0   0   0   0   1
s70   1   1   0   1   1   0   1   1   1   1   0   0   0   0   0   1   1   1   0
s71   1   0   0   0   0   1   0   0   0   1   0   0   1   0   1   0   0   0   1
s72   1   0   0   0   1   0   1   1   1   0   0   1   1   0   0   1   0   0   0
s73   1   0   1   0   1   1   1   1   1   0   0   1   0   0   0   0   1   1   0
s74   1   0   0   0   1   1   0   1   1   0   0   1   1   0   1   0   1   1   0
s75   1   0   0   0   1   0   0   0   1   0   0   0   0   0   0   0   1   1   1
s76   0   0   0   0   0   0   1   1   0   1   0   0   0   0   0   1   0   0   0
s77   0   1   1   1   1   0   1   1   0   0   1   1   0   1   1   0   1   1   0
s78   0   0   0   1   1   1   1   1   1   0   1   1   1   1   0   0   0   0   0
s79   1   1   0   1   1   0   1   1   0   0   0   0   0   0   0   1   0   0   1
s80   1   0   1   1   0   1   0   1   1   0   0   1   1   0   0   1   1   1   0
s81   1   0   0   0   1   1   1   1   1   1   0   0   1   0   1   0   1   1   1
s82   0   1   0   0   0   1   1   0   1   1   0   1   0   0   0   1   0   0   1
s83   0   1   0   1   0   0   0   0   1   0   1   0   1   1   0   1   1   1   1
    s60 s61 s62 s63 s64 s65 s66 s67 s68 s69 s70 s71 s72 s73 s74 s75 s76 s77 s78
s1    0   0   1   0   0   0   0   0   0   0   1   0   1   1   1   0   0   1   0
s2    0   0   0   1   0   1   0   1   1   1   0   0   1   0   1   0   1   1   1
s3    1   0   0   1   1   0   0   1   0   0   0   1   0   1   1   0   1   1   1
s4    0   0   0   1   0   1   1   0   0   1   0   1   1   1   1   1   1   1   1
s5    1   0   1   1   1   0   1   0   1   1   1   0   0   0   1   0   1   0   1
s6    1   0   0   0   0   0   0   1   1   1   0   1   0   1   0   0   0   1   1
s7    1   0   0   1   0   0   0   1   1   0   1   0   0   0   0   1   0   1   0
s8    0   1   1   1   1   0   1   0   1   0   1   1   0   0   0   0   0   0   1
s9    1   1   1   0   1   0   1   0   1   1   0   1   0   1   1   1   0   1   1
s10   1   1   0   0   0   1   1   1   1   0   1   0   0   0   0   1   0   0   0
s11   1   1   1   0   0   1   0   1   1   1   0   0   1   1   1   0   0   0   0
s12   0   0   1   0   0   0   0   0   1   1   0   1   1   1   1   1   0   1   0
s13   0   1   0   1   0   0   1   1   1   1   0   0   1   1   0   0   1   0   1
s14   1   1   0   1   1   0   1   1   1   1   1   0   1   1   0   0   0   0   1
s15   0   1   0   1   0   1   0   1   0   0   1   1   0   1   0   0   0   0   1
s16   1   0   1   0   1   0   1   1   0   0   0   0   1   1   1   1   0   0   1
s17   1   1   1   0   0   1   1   1   0   0   1   0   1   0   0   0   1   0   1
s18   0   0   1   1   1   1   1   0   1   0   0   1   1   0   0   1   1   1   1
s19   1   1   0   1   0   0   1   0   0   0   1   0   0   0   1   0   0   0   0
s20   1   1   1   0   0   0   1   1   0   0   0   1   1   1   1   1   0   0   1
s21   0   0   1   0   1   0   0   0   1   1   1   1   0   0   1   0   0   1   0
s22   0   1   1   0   1   1   0   0   0   1   0   0   1   0   0   1   1   1   0
s23   1   0   0   1   1   0   1   0   1   0   1   0   1   0   1   1   1   1   0
s24   1   0   1   1   0   0   0   1   1   1   0   1   1   1   1   1   1   0   0
s25   0   0   1   1   0   1   0   0   0   0   1   1   0   0   1   1   1   0   1
s26   0   1   0   1   0   0   0   1   0   1   1   0   1   0   1   0   1   0   0
s27   0   0   1   0   0   0   0   1   0   1   1   1   0   0   1   1   0   1   0
s28   0   1   0   0   0   0   1   1   1   0   1   0   1   0   1   0   1   0   1
s29   0   1   1   0   0   0   1   1   1   1   1   1   0   0   1   1   0   1   0
s30   0   0   0   1   0   0   1   1   0   0   1   0   1   0   0   1   0   0   0
s31   0   1   1   0   1   0   0   1   0   0   0   1   0   0   0   1   0   0   1
s32   0   1   0   1   0   0   1   0   1   0   1   1   1   1   0   1   0   1   1
s33   0   1   1   1   0   1   1   1   0   0   1   1   1   0   0   0   1   1   1
s34   0   1   0   0   0   0   1   1   1   0   1   1   1   1   0   1   1   1   0
s35   0   0   0   1   1   0   1   1   1   0   1   1   1   1   0   0   0   1   1
s36   1   0   0   0   1   0   1   1   1   0   0   0   0   1   0   1   0   1   1
s37   0   0   1   1   1   0   0   1   0   0   0   0   0   1   1   0   0   0   0
s38   1   1   1   0   1   1   0   0   1   1   0   1   1   0   0   0   0   1   0
s39   0   0   0   0   1   1   0   1   0   0   1   1   1   0   1   0   0   0   0
s40   1   0   0   0   1   0   1   1   0   0   0   1   1   1   1   0   0   1   0
s41   1   0   0   1   1   0   1   0   0   0   1   1   1   1   1   1   0   0   0
s42   1   1   0   1   1   0   1   1   1   1   1   0   0   0   0   0   0   1   0
s43   1   0   0   1   0   1   1   0   1   0   0   0   0   1   0   0   0   1   0
s44   1   1   0   1   1   1   0   1   0   1   1   0   0   0   0   0   0   1   1
s45   0   1   1   1   1   0   0   1   0   0   1   0   1   1   1   1   0   1   1
s46   1   1   1   1   1   1   0   1   1   1   0   1   0   1   1   0   0   0   1
s47   0   0   0   1   0   1   0   0   1   1   1   0   1   1   0   0   1   1   1
s48   1   1   1   1   1   0   0   1   1   1   1   0   1   1   1   0   1   1   1
s49   1   1   0   0   0   0   1   0   1   1   1   0   1   1   1   1   0   0   1
s50   0   1   0   1   1   0   0   1   1   1   1   1   0   0   0   0   1   0   0
s51   1   0   0   0   0   0   0   1   1   0   0   0   0   0   0   0   0   1   1
s52   1   0   1   0   0   0   0   1   1   1   0   0   1   1   1   0   0   1   1
s53   0   0   0   0   0   0   0   1   0   0   0   1   1   0   1   0   0   0   1
s54   0   0   1   0   1   1   1   1   1   1   0   0   0   0   0   0   0   1   1
s55   1   1   1   1   0   1   1   0   0   0   0   1   0   0   1   0   0   1   0
s56   0   1   0   1   0   1   1   0   0   0   1   0   1   0   0   0   1   0   0
s57   1   1   1   1   1   0   0   1   1   0   1   0   0   1   1   1   0   1   0
s58   1   1   1   1   1   0   0   1   1   0   1   0   0   1   1   1   0   1   0
s59   1   1   0   1   1   1   0   0   1   1   0   1   0   0   0   1   0   0   0
s60   0   1   1   1   1   1   1   0   1   1   1   1   1   1   1   0   0   0   0
s61   1   0   0   1   1   0   1   1   1   0   0   0   0   0   1   0   1   0   0
s62   1   0   0   1   1   0   0   1   1   1   0   0   0   0   1   0   0   1   0
s63   1   1   1   0   1   0   1   1   1   1   0   0   0   0   1   1   1   0   0
s64   1   1   1   1   0   0   1   1   0   1   1   1   1   1   1   0   0   1   0
s65   1   0   0   0   0   0   1   1   0   1   0   0   0   1   0   0   0   0   1
s66   1   1   0   1   1   1   0   0   0   0   0   1   0   1   1   1   0   0   1
s67   0   1   1   1   1   1   0   0   1   0   0   1   1   0   0   0   1   0   0
s68   1   1   1   1   0   0   0   1   0   1   0   1   0   1   1   1   1   1   1
s69   1   0   1   1   1   1   0   0   1   0   1   1   1   1   1   1   1   1   1
s70   1   0   0   0   1   0   0   0   0   1   0   0   0   0   0   1   1   1   0
s71   1   0   0   0   1   0   1   1   1   1   0   0   0   0   0   1   0   0   0
s72   1   0   0   0   1   0   0   1   0   1   0   0   0   0   1   0   0   0   0
s73   1   0   0   0   1   1   1   0   1   1   0   0   0   0   0   1   1   0   1
s74   1   1   1   1   1   0   1   0   1   1   0   0   1   0   0   1   0   0   1
s75   0   0   0   1   0   0   1   0   1   1   1   1   0   1   1   0   0   0   1
s76   0   1   0   1   0   0   0   1   1   1   1   0   0   1   0   0   0   0   1
s77   0   0   1   0   1   0   0   0   1   1   1   0   0   0   0   0   0   0   0
s78   0   0   0   0   0   1   1   0   1   1   0   0   0   1   1   1   1   0   0
s79   0   0   1   1   1   0   0   0   0   0   0   0   0   1   0   0   0   1   1
s80   0   0   1   1   0   0   0   1   1   1   0   1   1   0   1   0   1   0   1
s81   1   0   1   0   1   1   0   0   1   1   1   1   0   0   0   1   0   1   1
s82   0   0   0   0   1   0   1   1   0   1   0   1   1   0   0   1   1   0   1
s83   0   0   1   1   1   0   0   0   0   0   0   0   1   0   0   0   0   1   1
    s79 s80 s81 s82 s83
s1    0   0   0   1   0
s2    0   1   0   1   0
s3    0   0   1   0   1
s4    0   1   1   0   1
s5    0   0   1   1   0
s6    1   1   0   1   1
s7    1   0   1   0   0
s8    0   0   0   0   0
s9    1   0   1   1   0
s10   0   1   0   1   0
s11   0   1   0   1   1
s12   0   1   1   1   1
s13   0   0   0   0   1
s14   0   1   0   0   0
s15   1   0   0   0   0
s16   1   1   1   1   0
s17   1   1   0   0   0
s18   0   1   1   0   1
s19   0   0   1   0   0
s20   1   1   0   0   0
s21   1   0   0   1   1
s22   1   0   1   0   0
s23   1   1   1   0   1
s24   1   0   1   0   0
s25   0   0   0   1   0
s26   1   0   1   0   1
s27   0   1   0   1   0
s28   1   1   1   1   0
s29   1   0   0   1   1
s30   0   1   0   1   1
s31   0   1   1   0   1
s32   0   1   0   1   1
s33   0   0   0   1   0
s34   1   1   1   1   0
s35   0   0   0   1   1
s36   0   1   1   0   1
s37   0   0   1   0   0
s38   1   0   1   0   1
s39   0   0   0   1   0
s40   1   1   0   0   1
s41   1   1   1   0   0
s42   1   0   0   1   1
s43   0   1   0   0   0
s44   1   1   0   0   1
s45   1   0   1   0   0
s46   0   1   1   1   0
s47   1   0   1   1   0
s48   1   1   1   0   0
s49   0   1   1   1   1
s50   0   0   1   1   0
s51   0   0   0   0   1
s52   0   1   0   1   0
s53   0   1   1   0   1
s54   0   0   0   0   1
s55   0   0   1   0   0
s56   1   1   0   1   1
s57   0   1   1   0   1
s58   0   1   1   0   1
s59   1   0   1   1   1
s60   0   0   1   0   0
s61   0   0   0   0   0
s62   1   1   1   0   1
s63   1   1   0   0   1
s64   1   0   1   1   1
s65   0   0   1   0   0
s66   0   0   0   1   0
s67   0   1   0   1   0
s68   0   1   1   0   0
s69   0   1   1   1   0
s70   0   0   1   0   0
s71   0   1   1   1   0
s72   0   1   0   1   1
s73   1   0   0   0   0
s74   0   1   0   0   0
s75   0   0   1   1   0
s76   0   1   0   1   0
s77   1   0   1   0   1
s78   1   1   1   1   1
s79   0   0   0   0   0
s80   0   0   1   0   0
s81   0   1   0   0   1
s82   0   0   0   0   0
s83   0   0   1   0   0
> net[x] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x])
Warning message:
In net[x] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x]) :
  number of items to replace is not a multiple of replacement length
>     nets[[x]] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x])
> foo <- function(par) {
+         q01 <- exp(par[1])
+         q10 <- exp(par[2])
+         pSpec <- exp(par[3])
+         fitPhyloNetwork(tt[[x]], net[[x]], c(q01[x], q10[x], pSpec[x]))
+     }
> foo(log(c(q01[x], q10[x], pSpec[x])))
Error in network[mm, mm] (from fitPhyloNetwork.R#13) : incorrect number of dimensions
>         fitPhyloNetwork(tt[[x]], net[[x]], c(q01[x], q10[x], pSpec[x]))
Error in network[mm, mm] (from fitPhyloNetwork.R#13) : incorrect number of dimensions
>         fitPhyloNetwork(tt[[x]], nets[[x]], c(q01[x], q10[x], pSpec[x]))
[1] 2.189536
>     out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
Error in network[mm, mm] (from fitPhyloNetwork.R#13) : incorrect number of dimensions
> foo <- function(par) {
+         q01 <- exp(par[1])
+         q10 <- exp(par[2])
+         pSpec <- exp(par[3])
+         fitPhyloNetwork(tt[[x]], nets[[x]], c(q01[x], q10[x], pSpec[x]))
+     }
>     out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
  Nelder-Mead direct search function minimizer
function value for initial parameters = 2.079442
  Scaled convergence tolerance is 3.09861e-08
Stepsize computed as 0.069315
BUILD              4 2.107254 2.053037
  C-c C-c
> sym.trans <- function(x) {
+     pSpec[x] <- runif(1, sProb[1], sProb[2])
+     q01[x] <- runif(1, qProb[1], qProb[2])
+     q10[x] <- q01[x]
+     lambda[x] <- runif(1, spProb[1], spProb[2])
+     ntaxa[x] <- round(runif(1, tProb[1], tProb[2]))
+
+
+     tt[[x]] <- sim.bdtree(b = lambda[x], d = 0, stop = "taxa", n = ntaxa[x])
+     tt[[x]] <- drop.tip(tt[[x]], paste0("s", ntaxa[x]))
+     nets[[x]] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x])
+
+
+     foo <- function(par) {
+         q01 <- exp(par[1])
+         q10 <- exp(par[2])
+         pSpec <- exp(par[3])
+         fitPhyloNetwork(tt[[x]], nets[[x]], c(q01[x], q10[x], pSpec[x]))
+     }
+
+     #foo(log(c(q01[x], q10[x], pSpec[x])))
+     out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
+     res <- c(x, out$value, exp(out$par), q01[x], q10[x], pSpec[x])
+     #exp(out$par)
+ }
> sym.trans(1)
  Nelder-Mead direct search function minimizer
function value for initial parameters = 510.812880
  Scaled convergence tolerance is 7.61171e-06
Stepsize computed as 0.069315
BUILD              4 510.812880 508.336701
EXTENSION          6 509.853089 505.144851
EXTENSION          8 508.374599 502.781028
EXTENSION         10 508.336701 500.133707
EXTENSION         12 505.144851 493.319075
LO-REDUCTION      14 502.781028 493.319075
REFLECTION        16 500.133707 492.063897
EXTENSION         18 494.003578 487.340891
EXTENSION         20 493.319075 485.089832
HI-REDUCTION      22 492.063897 485.089832
REFLECTION        24 488.627115 484.649417
HI-REDUCTION      26 487.340891 484.649417
EXTENSION         28 485.872513 479.699161
LO-REDUCTION      30 485.089832 479.699161
LO-REDUCTION      32 484.649417 479.699161
EXTENSION         34 482.508077 475.549010
EXTENSION         36 481.916110 474.397874
EXTENSION         38 479.699161 473.965134
LO-REDUCTION      40 475.549010 473.965134
LO-REDUCTION      42 474.397874 473.883672
LO-REDUCTION      44 474.220721 473.842500
HI-REDUCTION      46 473.965134 473.842500
HI-REDUCTION      48 473.883672 473.830467
HI-REDUCTION      50 473.875786 473.827410
HI-REDUCTION      52 473.842500 473.817543
LO-REDUCTION      54 473.830467 473.817005
HI-REDUCTION      56 473.827410 473.810492
LO-REDUCTION      58 473.817543 473.810492
LO-REDUCTION      60 473.817005 473.810492
HI-REDUCTION      62 473.814843 473.810492
LO-REDUCTION      64 473.812057 473.809732
HI-REDUCTION      66 473.811010 473.809732
LO-REDUCTION      68 473.810492 473.809732
LO-REDUCTION      70 473.810170 473.809732
HI-REDUCTION      72 473.809896 473.809714
HI-REDUCTION      74 473.809760 473.809670
HI-REDUCTION      76 473.809732 473.809648
HI-REDUCTION      78 473.809714 473.809647
HI-REDUCTION      80 473.809670 473.809644
HI-REDUCTION      82 473.809648 473.809635
HI-REDUCTION      84 473.809647 473.809632
HI-REDUCTION      86 473.809644 473.809632
Exiting from Nelder Mead minimizer
    88 function evaluations used
> sym.trans <- function(x) {
+     pSpec[x] <- runif(1, sProb[1], sProb[2])
+     q01[x] <- runif(1, qProb[1], qProb[2])
+     q10[x] <- q01[x]
+     lambda[x] <- runif(1, spProb[1], spProb[2])
+     ntaxa[x] <- round(runif(1, tProb[1], tProb[2]))
+
+
+     tt[[x]] <- sim.bdtree(b = lambda[x], d = 0, stop = "taxa", n = ntaxa[x])
+     tt[[x]] <- drop.tip(tt[[x]], paste0("s", ntaxa[x]))
+     nets[[x]] <- simPhyloNetwork(tt[[x]], qRate = q01[x], sProb = pSpec[x])
+
+
+     foo <- function(par) {
+         q01 <- exp(par[1])
+         q10 <- exp(par[2])
+         pSpec <- exp(par[3])
+         fitPhyloNetwork(tt[[x]], nets[[x]], c(q01[x], q10[x], pSpec[x]))
+     }
+
+     #foo(log(c(q01[x], q10[x], pSpec[x])))
+     out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
+     res <- c(x, out$value, exp(out$par), q01[x], q10[x], pSpec[x])
+     return(res)
+ }
> sym.trans(1)
  Nelder-Mead direct search function minimizer
function value for initial parameters = 8.465464
  Scaled convergence tolerance is 1.26145e-07
Stepsize computed as 0.069315
BUILD              4 8.503390 7.826826
LO-REDUCTION       6 8.465464 7.826826
EXTENSION          8 8.437858 7.334512
EXTENSION         10 7.988164 6.292663
LO-REDUCTION      12 7.826826 6.292663
EXTENSION         14 7.334512 4.474284
EXTENSION         16 6.317620 3.153755
LO-REDUCTION      18 6.292663 3.153755
LO-REDUCTION      20 4.474284 3.153755
LO-REDUCTION      22 3.243771 3.153755
HI-REDUCTION      24 3.234777 3.153755
HI-REDUCTION      26 3.224851 3.153755
HI-REDUCTION      28 3.218837 3.153755
HI-REDUCTION      30 3.212981 3.153755
HI-REDUCTION      32 3.209052 3.153755
LO-REDUCTION      34 3.205509 3.153755
HI-REDUCTION      36 3.189064 3.153755
EXTENSION         38 3.179984 3.121461
LO-REDUCTION      40 3.174020 3.121461
EXTENSION         42 3.153755 3.043792
EXTENSION         44 3.134728 2.986090
EXTENSION         46 3.121461 2.950496
REFLECTION        48 3.043792 2.942081
LO-REDUCTION      50 2.986090 2.941932
LO-REDUCTION      52 2.950496 2.941932
HI-REDUCTION      54 2.949182 2.941932
LO-REDUCTION      56 2.942359 2.941932
HI-REDUCTION      58 2.942081 2.941511
HI-REDUCTION      60 2.942020 2.941500
HI-REDUCTION      62 2.941932 2.941436
HI-REDUCTION      64 2.941511 2.941436
HI-REDUCTION      66 2.941500 2.941383
LO-REDUCTION      68 2.941457 2.941379
HI-REDUCTION      70 2.941436 2.941352
LO-REDUCTION      72 2.941383 2.941350
EXTENSION         74 2.941379 2.941315
HI-REDUCTION      76 2.941352 2.941315
EXTENSION         78 2.941350 2.941293
EXTENSION         80 2.941340 2.941225
EXTENSION         82 2.941315 2.941169
EXTENSION         84 2.941293 2.941092
EXTENSION         86 2.941225 2.940856
EXTENSION         88 2.941169 2.940625
EXTENSION         90 2.941092 2.940365
EXTENSION         92 2.940856 2.939693
EXTENSION         94 2.940625 2.938928
EXTENSION         96 2.940365 2.938044
EXTENSION         98 2.939693 2.936335
EXTENSION        100 2.938928 2.935351
EXTENSION        102 2.938044 2.933493
EXTENSION        104 2.936335 2.931363
LO-REDUCTION     106 2.935351 2.931363
LO-REDUCTION     108 2.933493 2.931363
LO-REDUCTION     110 2.933197 2.931363
EXTENSION        112 2.931599 2.929504
LO-REDUCTION     114 2.931561 2.929504
REFLECTION       116 2.931363 2.929297
EXTENSION        118 2.929653 2.928618
LO-REDUCTION     120 2.929504 2.928618
HI-REDUCTION     122 2.929297 2.928618
EXTENSION        124 2.928890 2.928567
LO-REDUCTION     126 2.928700 2.928567
REFLECTION       128 2.928667 2.928566
LO-REDUCTION     130 2.928618 2.928566
LO-REDUCTION     132 2.928578 2.928566
LO-REDUCTION     134 2.928575 2.928565
HI-REDUCTION     136 2.928568 2.928565
HI-REDUCTION     138 2.928567 2.928565
LO-REDUCTION     140 2.928566 2.928565
HI-REDUCTION     142 2.928566 2.928565
HI-REDUCTION     144 2.928565 2.928565
REFLECTION       146 2.928565 2.928565
LO-REDUCTION     148 2.928565 2.928565
Exiting from Nelder Mead minimizer
    150 function evaluations used
[1] 1.000000e+00 2.928565e+00 3.118902e-06 7.159291e-01 1.034861e+08
[6] 8.400873e-01 8.400873e-01 1.663356e+00
> registerDoMC(2)
> test <- as.list(1:2)
> result <- ldply(test, sym.trans, .parallel = TRUE)
  Nelder-Mead direct search function minimizer
  Nelder-Mead direct search function minimizer
function value for initial parameters = 8.317766
  Scaled convergence tolerance is 1.23944e-07
Stepsize computed as 0.069315
BUILD              4 8.319919 8.207867
LO-REDUCTION       6 8.317766 8.207867
EXTENSION          8 8.315622 8.163095
REFLECTION        10 8.236060 8.151756
LO-REDUCTION      12 8.207867 8.151756
LO-REDUCTION      14 8.163095 8.151756
HI-REDUCTION      16 8.154172 8.151756
HI-REDUCTION      18 8.154113 8.151683
LO-REDUCTION      20 8.152675 8.151683
HI-REDUCTION      22 8.152181 8.151683
LO-REDUCTION      24 8.151924 8.151683
REFLECTION        26 8.151756 8.151495
REFLECTION        28 8.151687 8.151440
HI-REDUCTION      30 8.151683 8.151440
EXTENSION         32 8.151511 8.151169
LO-REDUCTION      34 8.151495 8.151169
EXTENSION         36 8.151440 8.150680
EXTENSION         38 8.151197 8.150513
LO-REDUCTION      40 8.151169 8.150513
EXTENSION         42 8.150686 8.149440
LO-REDUCTION      44 8.150680 8.149440
LO-REDUCTION      46 8.150513 8.149440
EXTENSION         48 8.149622 8.147040
LO-REDUCTION      50 8.149455 8.147040
EXTENSION         52 8.149440 8.145078
EXTENSION         54 8.148102 8.139374
LO-REDUCTION      56 8.147040 8.139374
EXTENSION         58 8.145078 8.121869
EXTENSION         60 8.139391 8.100594
REFLECTION        62 8.139374 8.097513
HI-REDUCTION      64 8.124594 8.097513
LO-REDUCTION      66 8.121869 8.097513
HI-REDUCTION      68 8.109401 8.097513
LO-REDUCTION      70 8.100594 8.097513
EXTENSION         72 8.100002 8.095781
EXTENSION         74 8.098500 8.092715
EXTENSION         76 8.097513 8.084743
HI-REDUCTION      78 8.095781 8.084743
EXTENSION         80 8.092877 8.079156
EXTENSION         82 8.092715 8.073534
EXTENSION         84 8.084743 8.062498
EXTENSION         86 8.079156 8.039151
REFLECTION        88 8.073534 8.035290
LO-REDUCTION      90 8.062498 8.035290
REFLECTION        92 8.039151 8.020355
EXTENSION         94 8.035830 8.003031
HI-REDUCTION      96 8.035290 8.003031
REFLECTION        98 8.020355 7.993988
EXTENSION        100 8.017561 7.981122
LO-REDUCTION     102 8.003031 7.981122
EXTENSION        104 7.993988 7.954083
LO-REDUCTION     106 7.986926 7.954083
HI-REDUCTION     108 7.981122 7.954083
REFLECTION       110 7.965854 7.944921
EXTENSION        112 7.958500 7.927963
LO-REDUCTION     114 7.954083 7.927963
LO-REDUCTION     116 7.944921 7.927451
EXTENSION        118 7.933475 7.910541
EXTENSION        120 7.927963 7.902380
LO-REDUCTION     122 7.927451 7.902380
EXTENSION        124 7.910541 7.890255
HI-REDUCTION     126 7.903465 7.890255
LO-REDUCTION     128 7.902380 7.890255
REFLECTION       130 7.900110 7.887288
HI-REDUCTION     132 7.893262 7.887288
HI-REDUCTION     134 7.891228 7.887288
HI-REDUCTION     136 7.890255 7.887288
HI-REDUCTION     138 7.887756 7.887027
HI-REDUCTION     140 7.887488 7.886378
HI-REDUCTION     142 7.887288 7.886378
HI-REDUCTION     144 7.887027 7.886373
LO-REDUCTION     146 7.886502 7.886177
EXTENSION        148 7.886378 7.885223
LO-REDUCTION     150 7.886373 7.885223
LO-REDUCTION     152 7.886177 7.885223
REFLECTION       154 7.885712 7.885067
EXTENSION        156 7.885494 7.884697
EXTENSION        158 7.885223 7.884072
EXTENSION        160 7.885067 7.882934
EXTENSION        162 7.884697 7.881801
LO-REDUCTION     164 7.884072 7.881801
EXTENSION        166 7.882934 7.879817
LO-REDUCTION     168 7.882247 7.879817
HI-REDUCTION     170 7.881801 7.879817
EXTENSION        172 7.880897 7.877675
LO-REDUCTION     174 7.880101 7.877675
EXTENSION        176 7.879817 7.876356
REFLECTION       178 7.878122 7.875785
LO-REDUCTION     180 7.877675 7.875785
LO-REDUCTION     182 7.876751 7.875785
EXTENSION        184 7.876356 7.875095
LO-REDUCTION     186 7.876336 7.875095
REFLECTION       188 7.875785 7.874827
EXTENSION        190 7.875303 7.873902
LO-REDUCTION     192 7.875095 7.873902
LO-REDUCTION     194 7.874827 7.873902
EXTENSION        196 7.874650 7.873428
EXTENSION        198 7.874025 7.872943
LO-REDUCTION     200 7.873902 7.872943
REFLECTION       202 7.873428 7.872773
HI-REDUCTION     204 7.872996 7.872773
EXTENSION        206 7.872943 7.872664
LO-REDUCTION     208 7.872933 7.872662
REFLECTION       210 7.872773 7.872596
LO-REDUCTION     212 7.872664 7.872596
REFLECTION       214 7.872662 7.872550
LO-REDUCTION     216 7.872618 7.872540
HI-REDUCTION     218 7.872596 7.872540
LO-REDUCTION     220 7.872550 7.872534
REFLECTION       222 7.872550 7.872530
REFLECTION       224 7.872540 7.872519
HI-REDUCTION     226 7.872534 7.872519
REFLECTION       228 7.872530 7.872518
HI-REDUCTION     230 7.872528 7.872518
EXTENSION        232 7.872521 7.872502
LO-REDUCTION     234 7.872519 7.872502
EXTENSION        236 7.872518 7.872495
EXTENSION        238 7.872511 7.872480
EXTENSION        240 7.872502 7.872473
EXTENSION        242 7.872495 7.872461
LO-REDUCTION     244 7.872480 7.872461
LO-REDUCTION     246 7.872473 7.872461
EXTENSION        248 7.872465 7.872457
LO-REDUCTION     250 7.872465 7.872457
REFLECTION       252 7.872461 7.872455
EXTENSION        254 7.872457 7.872454
LO-REDUCTION     256 7.872457 7.872454
LO-REDUCTION     258 7.872455 7.872454
REFLECTION       260 7.872455 7.872454
LO-REDUCTION     262 7.872454 7.872454
LO-REDUCTION     264 7.872454 7.872454
LO-REDUCTION     266 7.872454 7.872454
Exiting from Nelder Mead minimizer
    268 function evaluations used
Error in do.ply(i) :
  task 2 failed - "missing value where TRUE/FALSE needed"
> result
Error: object 'result' not found
> getwd()
[1] "/Users/gburin/Dropbox/netphy"
> setwd("~/Documents/netphy/")
> tree.num <- 1:1000
> pSpec[] <- runif(1000, sProb[1], sProb[2])
> q01[] <- runif(1, qProb[1], qProb[2])
> lambda[] <- runif(1, spProb[1], spProb[2])
> ntaxa[] <- round(runif(1, tProb[1], tProb[2]))
> tree.num <- 1:1000
> pSpec[] <- runif(1000, sProb[1], sProb[2])
> q01[] <- runif(1000, qProb[1], qProb[2])
> lambda[] <- runif(1000, spProb[1], spProb[2])
> ntaxa[] <- round(runif(1000, tProb[1], tProb[2]))
> trees <- as.list(rep(NA, 1000))
> nets <- as.list(rep(NA, 1000))
> inputData <- data.frame(tree.num = tree.num, q01 = q01, q10 = q01, pSpec = pSpec, lambda = lambda, ntaxa = ntaxa)
> head(inputData)
  tree.num        q01        q10     pSpec    lambda ntaxa
1        1 0.99853587 0.99853587 0.6705248 1.7602146    15
2        2 0.02098267 0.02098267 0.5744425 1.2449539    81
3        3 0.58238361 0.58238361 0.7708589 0.7913905    27
4        4 0.87553305 0.87553305 0.5176874 1.4706315    88
5        5 0.06971977 0.06971977 0.6540870 1.4280854    55
6        6 0.23814964 0.23814964 0.5799347 0.3175819    15
> sym.trans <- function(pars) {
+     n <- pars[x, 1]
+     q01 <- pars[x, 2]
+     q10 <- pars[x, 3]
+     pSpec <- pars[x, 4]
+     lambda <- pars[x, 5]
+     ntaxa <- pars[x, 6]
+ 
+     tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
+     tt <- drop.tip(tt, paste0("s", ntaxa))
+     write.tree(tt, file = paste0("./trees/symtrans/tree_", n, "_symtrans.txt"))
+     nets <- simPhyloNetwork(tt, qRate = q01[x], sProb = pSpec)
+     write.table(nets, file = paste0("./nets/symtrans/net_", n, "_symtrans.txt"), sep = ",", row.names = TRUE, col.names = TRUE, quote = FALSE)
+ 
+     foo <- function(par) {
+         q01 <- exp(par[1])
+         q10 <- exp(par[2])
+         pSpec <- exp(par[3])
+         fitPhyloNetwork(tt, nets, c(q01, q10, pSpec))
+     }
+ 
+     #foo(log(c(q01[x], q10[x], pSpec[x])))
+     out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
+     res <- c(n, q01, q10, pSpec, lambda, ntaxa, exp(out$par), out$value)
+     return(res)
+ }
> sym.trans(1)
Error in pars[x, 1] (from #2) : incorrect number of dimensions
> sym.trans(pars[1, ])
Error in sym.trans(pars[1, ]) (from #2) : object 'pars' not found
> sym.trans(inputData[1, ])
  Nelder-Mead direct search function minimizer
function value for initial parameters = 51.306785
  Scaled convergence tolerance is 7.64531e-07
Stepsize computed as 0.069315
BUILD              4 51.306785 51.099739
EXTENSION          6 51.177017 50.844158
LO-REDUCTION       8 51.177005 50.844158
EXTENSION         10 51.099739 50.605557
EXTENSION         12 50.937962 50.289953
EXTENSION         14 50.844158 50.102850
LO-REDUCTION      16 50.605557 50.102850
REFLECTION        18 50.289953 50.040641
HI-REDUCTION      20 50.213765 50.040641
EXTENSION         22 50.140163 49.913113
LO-REDUCTION      24 50.102850 49.913113
LO-REDUCTION      26 50.040641 49.913113
LO-REDUCTION      28 49.980892 49.913113
EXTENSION         30 49.936691 49.876002
EXTENSION         32 49.934375 49.822681
LO-REDUCTION      34 49.913113 49.822681
LO-REDUCTION      36 49.876002 49.822681
REFLECTION        38 49.851088 49.818604
LO-REDUCTION      40 49.829513 49.815404
HI-REDUCTION      42 49.822681 49.815404
LO-REDUCTION      44 49.818604 49.814380
HI-REDUCTION      46 49.815693 49.814380
HI-REDUCTION      48 49.815404 49.813284
HI-REDUCTION      50 49.814495 49.813284
LO-REDUCTION      52 49.814380 49.813284
LO-REDUCTION      54 49.813749 49.813284
HI-REDUCTION      56 49.813390 49.813257
LO-REDUCTION      58 49.813291 49.813174
HI-REDUCTION      60 49.813284 49.813147
HI-REDUCTION      62 49.813257 49.813121
HI-REDUCTION      64 49.813174 49.813121
LO-REDUCTION      66 49.813147 49.813118
HI-REDUCTION      68 49.813141 49.813117
LO-REDUCTION      70 49.813121 49.813115
HI-REDUCTION      72 49.813118 49.813110
HI-REDUCTION      74 49.813117 49.813110
HI-REDUCTION      76 49.813115 49.813109
LO-REDUCTION      78 49.813110 49.813109
REFLECTION        80 49.813110 49.813108
HI-REDUCTION      82 49.813109 49.813108
HI-REDUCTION      84 49.813109 49.813108
Exiting from Nelder Mead minimizer
    86 function evaluations used
 [1]  1.0000000  0.9985359  0.9985359  0.6705248  1.7602146 15.0000000
 [7]  0.8402231  0.7303257  0.6888212 49.8131074
> registerDoMC(2)
> result <- ldply(.data = inputData[1:2, ], .variables = .(tree.num), .fun = sym.trans, .parallel = TRUE)
Error in do.ply(i) : 
  task 1 failed - "unused argument (.variables = list(tree.num))"
> result <- ldply(.data = inputData[1:2, ], .fun = sym.trans, .parallel = TRUE)
Error in do.ply(i) : task 1 failed - "incorrect number of dimensions"
> result <- ldply(.data = inputData[1:2, ], .variables = ~tree.num, .fun = sym.trans, .parallel = TRUE)
Error in do.ply(i) : 
  task 1 failed - "unused argument (.variables = ~tree.num)"
> result <- ldply(.data = inputData[1:2, ], .variables = ~ tree.num, .fun = sym.trans, .parallel = TRUE)
Error in do.ply(i) : 
  task 1 failed - "unused argument (.variables = ~tree.num)"
> sym.trans <- function(pars) {
+     n <- pars[, 1]
+     q01 <- pars[, 2]
+     q10 <- pars[, 3]
+     pSpec <- pars[, 4]
+     lambda <- pars[, 5]
+     ntaxa <- pars[, 6]
+ 
+     tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
+     tt <- drop.tip(tt, paste0("s", ntaxa))
+     write.tree(tt, file = paste0("./trees/symtrans/tree_", n, "_symtrans.txt"))
+     nets <- simPhyloNetwork(tt, qRate = q01[x], sProb = pSpec)
+     write.table(nets, file = paste0("./nets/symtrans/net_", n, "_symtrans.txt"), sep = ",", row.names = TRUE, col.names = TRUE, quote = FALSE)
+ 
+     foo <- function(par) {
+         q01 <- exp(par[1])
+         q10 <- exp(par[2])
+         pSpec <- exp(par[3])
+         fitPhyloNetwork(tt, nets, c(q01, q10, pSpec))
+     }
+ 
+     #foo(log(c(q01[x], q10[x], pSpec[x])))
+     out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
+     res <- c(n, q01, q10, pSpec, lambda, ntaxa, exp(out$par), out$value)
+     return(res)
+ }
> registerDoMC(2)
> result <- adply(.data = inputData[1:2, ], .margins = 1, .fun = sym.trans, .parallel = TRUE)
  Nelder-Mead direct search function minimizer
function value for initial parameters = 35.691529
  Scaled convergence tolerance is 5.31845e-07
Stepsize computed as 0.069315
BUILD              4 35.864094 35.691529
  Nelder-Mead direct search function minimizer
LO-REDUCTION       6 35.822771 35.691529
EXTENSION          8 35.792072 35.598392
EXTENSION         10 35.717959 35.421858
EXTENSION         12 35.691529 35.331347
EXTENSION         14 35.598392 35.248731
EXTENSION         16 35.421858 34.860329
LO-REDUCTION      18 35.331347 34.860329
LO-REDUCTION      20 35.248731 34.860329
EXTENSION         22 35.061264 34.725328
LO-REDUCTION      24 35.040587 34.725328
REFLECTION        26 34.860329 34.612557
HI-REDUCTION      28 34.740538 34.612557
LO-REDUCTION      30 34.725328 34.612557
REFLECTION        32 34.719829 34.584742
REFLECTION        34 34.624886 34.582021
REFLECTION        36 34.612557 34.580174
REFLECTION        38 34.584742 34.558228
HI-REDUCTION      40 34.582021 34.558228
LO-REDUCTION      42 34.580174 34.558228
LO-REDUCTION      44 34.562222 34.556985
LO-REDUCTION      46 34.558764 34.554531
HI-REDUCTION      48 34.558228 34.553717
HI-REDUCTION      50 34.556985 34.553717
LO-REDUCTION      52 34.554531 34.553554
LO-REDUCTION      54 34.554424 34.553342
HI-REDUCTION      56 34.553717 34.553342
HI-REDUCTION      58 34.553554 34.553203
HI-REDUCTION      60 34.553382 34.553203
HI-REDUCTION      62 34.553342 34.553203
LO-REDUCTION      64 34.553232 34.553193
LO-REDUCTION      66 34.553212 34.553169
HI-REDUCTION      68 34.553203 34.553155
HI-REDUCTION      70 34.553193 34.553155
HI-REDUCTION      72 34.553169 34.553155
LO-REDUCTION      74 34.553163 34.553152
HI-REDUCTION      76 34.553160 34.553152
REFLECTION        78 34.553155 34.553152
HI-REDUCTION      80 34.553152 34.553150
HI-REDUCTION      82 34.553152 34.553149
HI-REDUCTION      84 34.553152 34.553149
HI-REDUCTION      86 34.553150 34.553149
Exiting from Nelder Mead minimizer
    88 function evaluations used
function value for initial parameters = 578.821331
  Scaled convergence tolerance is 8.62511e-06
Stepsize computed as 0.069315
BUILD              4 602.360018 578.814663
  C-c C-c

Warning message:
In mclapply(argsList, FUN, mc.preschedule = preschedule, mc.set.seed = set.seed,  :
  scheduled cores 2 encountered errors in user code, all values of the jobs will be affected
> head(inputData)
  tree.num        q01        q10     pSpec    lambda ntaxa
1        1 0.99853587 0.99853587 0.6705248 1.7602146    15
2        2 0.02098267 0.02098267 0.5744425 1.2449539    81
3        3 0.58238361 0.58238361 0.7708589 0.7913905    27
4        4 0.87553305 0.87553305 0.5176874 1.4706315    88
5        5 0.06971977 0.06971977 0.6540870 1.4280854    55
6        6 0.23814964 0.23814964 0.5799347 0.3175819    15
> result <- adply(.data = inputData[c(1,6), ], .margins = 1, .fun = sym.trans, .parallel = TRUE)
  Nelder-Mead direct search function minimizer
  Nelder-Mead direct search function minimizer
function value for initial parameters = 37.683031
  Scaled convergence tolerance is 5.61521e-07
Stepsize computed as 0.069315
function value for initial parameters = 43.577315
  Scaled convergence tolerance is 6.49353e-07
Stepsize computed as 0.069315
BUILD              4 43.582274 43.242108
BUILD              4 38.445728 37.613276
EXTENSION          6 43.577315 43.049136
EXTENSION          6 37.881826 36.583590
EXTENSION          8 43.318138 42.535338
LO-REDUCTION       8 37.683031 36.583590
LO-REDUCTION      10 43.242108 42.535338
EXTENSION         10 37.613276 35.759278
EXTENSION         12 43.049136 42.226162
EXTENSION         12 36.622537 34.838163
LO-REDUCTION      14 36.583590 34.838163
REFLECTION        14 42.620484 42.030859
LO-REDUCTION      16 35.759278 34.838163
REFLECTION        16 42.535338 41.982310
LO-REDUCTION      18 35.346983 34.838163
LO-REDUCTION      18 42.226162 41.982310
EXTENSION         20 35.051287 34.160554
REFLECTION        20 42.160064 41.945029
LO-REDUCTION      22 34.966252 34.160554
EXTENSION         22 42.030859 41.794151
EXTENSION         24 34.838163 33.352367
EXTENSION         24 41.982310 41.571889
EXTENSION         26 41.945029 41.140534
EXTENSION         26 34.320241 32.449052
EXTENSION         28 34.160554 31.712245
LO-REDUCTION      28 41.794151 41.140534
REFLECTION        30 41.571889 40.978438
EXTENSION         30 33.352367 29.693210
REFLECTION        32 41.234700 40.683059
LO-REDUCTION      32 32.449052 29.693210
LO-REDUCTION      34 41.140534 40.683059
EXTENSION         34 31.712245 28.605071
EXTENSION         36 40.978438 40.302357
EXTENSION         36 30.653412 28.166439
REFLECTION        38 40.803981 40.296429
REFLECTION        38 29.693210 27.875809
LO-REDUCTION      40 40.683059 40.296429
LO-REDUCTION      40 28.605071 27.875809
REFLECTION        42 40.364032 40.132436
HI-REDUCTION      42 28.489616 27.875809
LO-REDUCTION      44 40.302357 40.130768
LO-REDUCTION      44 28.166439 27.875809
LO-REDUCTION      46 40.296429 40.130768
REFLECTION        46 28.103404 27.815517
LO-REDUCTION      48 40.139002 40.112314
REFLECTION        48 27.909365 27.700683
HI-REDUCTION      50 40.132436 40.099483
LO-REDUCTION      50 27.875809 27.700683
REFLECTION        52 40.130768 40.095251
LO-REDUCTION      52 27.815517 27.694880
LO-REDUCTION      54 40.112314 40.082562
LO-REDUCTION      54 27.750576 27.694880
LO-REDUCTION      56 40.099483 40.081682
REFLECTION        56 27.712844 27.662601
REFLECTION        58 40.095251 40.081068
LO-REDUCTION      58 27.700683 27.662601
REFLECTION        60 40.082562 40.074670
HI-REDUCTION      60 27.694880 27.662601
LO-REDUCTION      62 40.081682 40.070638
LO-REDUCTION      62 27.681971 27.662601
REFLECTION        64 40.081068 40.060692
LO-REDUCTION      64 27.679845 27.662601
LO-REDUCTION      66 40.074670 40.060692
LO-REDUCTION      66 27.671771 27.662601
LO-REDUCTION      68 40.070638 40.060692
EXTENSION         68 27.671145 27.659633
REFLECTION        70 40.062516 40.058123
REFLECTION        70 27.666906 27.657835
HI-REDUCTION      72 40.062254 40.056753
EXTENSION         72 27.662601 27.651388
HI-REDUCTION      74 40.060692 40.056250
LO-REDUCTION      74 27.659633 27.651388
LO-REDUCTION      76 27.657835 27.651388
REFLECTION        76 40.058123 40.056125
LO-REDUCTION      78 27.653295 27.651388
HI-REDUCTION      78 40.056753 40.055400
LO-REDUCTION      80 27.651913 27.651388
HI-REDUCTION      80 40.056250 40.054966
HI-REDUCTION      82 27.651546 27.651201
EXTENSION         82 40.056125 40.053506
HI-REDUCTION      84 27.651440 27.651201
HI-REDUCTION      84 40.055400 40.053506
HI-REDUCTION      86 27.651388 27.651201
EXTENSION         86 40.054966 40.051234
LO-REDUCTION      88 27.651219 27.651188
LO-REDUCTION      88 40.054549 40.051234
HI-REDUCTION      90 27.651211 27.651142
EXTENSION         90 40.053506 40.047733
HI-REDUCTION      92 27.651201 27.651142
EXTENSION         92 40.052570 40.046266
HI-REDUCTION      94 27.651188 27.651142
EXTENSION         94 40.051234 40.040591
LO-REDUCTION      96 27.651151 27.651134
EXTENSION         96 40.047733 40.029619
REFLECTION        98 27.651145 27.651127
LO-REDUCTION      98 40.046266 40.029619
HI-REDUCTION     100 27.651142 27.651127
EXTENSION        100 40.040591 40.017131
HI-REDUCTION     102 27.651134 27.651127
EXTENSION        102 40.036524 40.004475
HI-REDUCTION     104 27.651129 27.651127
EXTENSION        104 40.029619 39.990484
HI-REDUCTION     106 27.651128 27.651126
REFLECTION       106 40.017131 39.986888
HI-REDUCTION     108 27.651127 27.651125
REFLECTION       108 40.004475 39.985187
HI-REDUCTION     110 27.651127 27.651125
LO-REDUCTION     110 39.990484 39.985187
HI-REDUCTION     112 27.651126 27.651125
LO-REDUCTION     112 39.986888 39.985187
LO-REDUCTION     114 27.651125 27.651125
HI-REDUCTION     114 39.986300 39.984337
Exiting from Nelder Mead minimizer
    116 function evaluations used
REFLECTION       116 39.985330 39.983981
LO-REDUCTION     118 39.985187 39.983981
HI-REDUCTION     120 39.984337 39.983981
HI-REDUCTION     122 39.984226 39.983904
LO-REDUCTION     124 39.984163 39.983856
LO-REDUCTION     126 39.983981 39.983852
HI-REDUCTION     128 39.983904 39.983838
HI-REDUCTION     130 39.983856 39.983810
HI-REDUCTION     132 39.983852 39.983810
HI-REDUCTION     134 39.983838 39.983810
LO-REDUCTION     136 39.983815 39.983805
HI-REDUCTION     138 39.983812 39.983803
HI-REDUCTION     140 39.983810 39.983802
HI-REDUCTION     142 39.983805 39.983802
LO-REDUCTION     144 39.983803 39.983801
HI-REDUCTION     146 39.983803 39.983801
HI-REDUCTION     148 39.983802 39.983801
HI-REDUCTION     150 39.983801 39.983801
Exiting from Nelder Mead minimizer
    152 function evaluations used
> result
  tree.num       q01       q10     pSpec    lambda ntaxa V1        V2        V3
1        1 0.9985359 0.9985359 0.6705248 1.7602146    15  1 0.9985359 0.9985359
2        6 0.2381496 0.2381496 0.5799347 0.3175819    15  6 0.2381496 0.2381496
         V4        V5 V6        V7         V8        V9      V10
1 0.6705248 1.7602146 15 1.1973480 0.96778952 0.9075209 39.98380
2 0.5799347 0.3175819 15 0.1099539 0.08747407 0.6018569 27.65112
> inputData <- data.frame(tree.num = tree.num, q01.sim = q01, q10.sim = q01, pSpec.sim = pSpec, lambda.sim = lambda, ntaxa = ntaxa)
> sym.trans <- function(pars) {
+     n <- pars[, 1]
+     q01 <- pars[, 2]
+     q10 <- pars[, 3]
+     pSpec <- pars[, 4]
+     lambda <- pars[, 5]
+     ntaxa <- pars[, 6]
+ 
+     tt <- sim.bdtree(b = lambda, d = 0, stop = "taxa", n = ntaxa)
+     tt <- drop.tip(tt, paste0("s", ntaxa))
+     write.tree(tt, file = paste0("./trees/symtrans/tree_", n, "_symtrans.txt"))
+     nets <- simPhyloNetwork(tt, qRate = q01[x], sProb = pSpec)
+     write.table(nets, file = paste0("./nets/symtrans/net_", n, "_symtrans.txt"), sep = ",", row.names = TRUE, col.names = TRUE, quote = FALSE)
+ 
+     foo <- function(par) {
+         q01 <- exp(par[1])
+         q10 <- exp(par[2])
+         pSpec <- exp(par[3])
+         fitPhyloNetwork(tt, nets, c(q01, q10, pSpec))
+     }
+ 
+     #foo(log(c(q01[x], q10[x], pSpec[x])))
+     out <- optim(log(c(0.5, 0.5, 0.5)), foo, control = list(trace = 6))
+     res <- c(exp(out$par), out$value)
+     names(res) <- c("q01.fit", "q10.fit", "pSpec.fit", "llik.fit")
+     return(res)
+ }
> result <- adply(.data = inputData[c(1,6), ], .margins = 1, .fun = sym.trans, .parallel = TRUE)
  Nelder-Mead direct search function minimizer
  Nelder-Mead direct search function minimizer
function value for initial parameters = 48.755027
  Scaled convergence tolerance is 7.26507e-07
Stepsize computed as 0.069315
function value for initial parameters = 57.990425
  Scaled convergence tolerance is 8.64125e-07
Stepsize computed as 0.069315
BUILD              4 48.853282 48.530975
BUILD              4 58.293131 57.990425
EXTENSION          6 48.755027 48.369540
REFLECTION         6 58.049744 57.884282
EXTENSION          8 48.667522 48.095755
EXTENSION          8 58.014874 57.779354
EXTENSION         10 48.530975 47.829792
EXTENSION         10 57.990425 57.575616
LO-REDUCTION      12 48.369540 47.829792
EXTENSION         12 57.884282 57.410518
EXTENSION         14 48.095755 47.564839
EXTENSION         14 57.779354 56.979594
LO-REDUCTION      16 47.855399 47.564839
LO-REDUCTION      16 57.575616 56.979594
LO-REDUCTION      18 47.829792 47.564839
REFLECTION        18 57.410518 56.832211
EXTENSION         20 57.050350 56.444335
HI-REDUCTION      20 47.645562 47.564839
HI-REDUCTION      22 47.627561 47.564839
LO-REDUCTION      22 56.979594 56.444335
EXTENSION         24 47.600676 47.529437
LO-REDUCTION      24 56.832211 56.444335
LO-REDUCTION      26 47.588059 47.529437
HI-REDUCTION      26 56.673736 56.444335
REFLECTION        28 47.564839 47.523769
LO-REDUCTION      28 56.622109 56.444335
LO-REDUCTION      30 47.558623 47.523769
EXTENSION         30 56.615629 56.370485
EXTENSION         32 56.481437 56.129562
EXTENSION         32 47.539881 47.487531
EXTENSION         34 56.444335 56.111863
EXTENSION         34 47.529437 47.446296
EXTENSION         36 56.370485 55.742558
EXTENSION         36 47.523769 47.426868
LO-REDUCTION      38 56.129562 55.742558
EXTENSION         38 47.487531 47.367825
LO-REDUCTION      40 56.111863 55.742558
EXTENSION         40 47.446296 47.270916
REFLECTION        42 55.833596 55.663172
LO-REDUCTION      42 47.426868 47.270916
LO-REDUCTION      44 55.812762 55.663172
REFLECTION        44 47.367825 47.254676
LO-REDUCTION      46 55.742558 55.663172
LO-REDUCTION      46 47.315926 47.254676
HI-REDUCTION      48 55.713558 55.663172
EXTENSION         48 47.270916 47.204913
REFLECTION        50 55.690848 55.660613
LO-REDUCTION      50 47.256253 47.204913
LO-REDUCTION      52 55.682362 55.659184
LO-REDUCTION      52 47.254676 47.204913
EXTENSION         54 55.663172 55.626102
LO-REDUCTION      54 47.211328 47.204913
LO-REDUCTION      56 55.660613 55.626102
REFLECTION        56 47.207468 47.203563
LO-REDUCTION      58 55.659184 55.626102
REFLECTION        58 47.205318 47.189076
REFLECTION        60 55.639783 55.620424
HI-REDUCTION      60 47.204913 47.189076
LO-REDUCTION      62 55.634942 55.620424
LO-REDUCTION      62 47.203563 47.189076
LO-REDUCTION      64 55.627305 55.620424
HI-REDUCTION      64 47.192996 47.189076
LO-REDUCTION      66 55.626102 55.620424
HI-REDUCTION      66 47.192902 47.189076
LO-REDUCTION      68 55.621685 55.620424
LO-REDUCTION      68 47.190686 47.189063
HI-REDUCTION      70 55.621647 55.620424
LO-REDUCTION      70 47.190684 47.189063
LO-REDUCTION      72 55.620704 55.620424
REFLECTION        72 47.189144 47.188156
HI-REDUCTION      74 55.620665 55.620424
LO-REDUCTION      74 47.189076 47.188156
LO-REDUCTION      76 55.620571 55.620423
LO-REDUCTION      76 47.189063 47.188156
HI-REDUCTION      78 55.620445 55.620412
LO-REDUCTION      78 47.188377 47.188156
HI-REDUCTION      80 55.620424 55.620384
REFLECTION        80 47.188220 47.187910
HI-REDUCTION      82 55.620423 55.620382
HI-REDUCTION      82 47.188188 47.187910
LO-REDUCTION      84 55.620412 55.620379
LO-REDUCTION      84 47.188156 47.187910
LO-REDUCTION      86 55.620384 55.620373
LO-REDUCTION      86 47.188022 47.187899
LO-REDUCTION      88 55.620382 55.620371
REFLECTION        88 47.187938 47.187865
LO-REDUCTION      90 55.620379 55.620370
REFLECTION        90 47.187910 47.187862
HI-REDUCTION      92 55.620373 55.620369
LO-REDUCTION      92 47.187899 47.187847
HI-REDUCTION      94 55.620371 55.620368
HI-REDUCTION      94 47.187865 47.187847
HI-REDUCTION      96 55.620370 55.620367
HI-REDUCTION      96 47.187862 47.187847
HI-REDUCTION      98 55.620369 55.620367
HI-REDUCTION      98 47.187856 47.187845
Exiting from Nelder Mead minimizer
    100 function evaluations used
LO-REDUCTION     100 47.187848 47.187844
REFLECTION       102 47.187847 47.187840
HI-REDUCTION     104 47.187845 47.187840
HI-REDUCTION     106 47.187844 47.187840
LO-REDUCTION     108 47.187842 47.187840
HI-REDUCTION     110 47.187841 47.187840
HI-REDUCTION     112 47.187841 47.187840
Exiting from Nelder Mead minimizer
    114 function evaluations used
> result
  tree.num   q01.sim   q10.sim pSpec.sim lambda.sim ntaxa   q01.fit   q10.fit
1        1 0.9985359 0.9985359 0.6705248  1.7602146    15 0.8779203 1.2683317
2        6 0.2381496 0.2381496 0.5799347  0.3175819    15 0.1848427 0.1718851
  pSpec.fit llik.fit
1 0.5409234 47.18784
2 0.3301534 55.62037
> 